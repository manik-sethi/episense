{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=16):\n",
    "        super(CNNUnit, self).__init__()\n",
    "        \n",
    "        # First 3x3 Conv Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        \n",
    "        # Second 3x3 Conv Layer \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu2 = nn.GELU()\n",
    "\n",
    "        # Third 3x3 Conv Layer with Stride 2 (Downsampling)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu3 = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # First Conv2D + GELU + BatchNorm\n",
    "        x = self.gelu1(self.bn1(self.conv1(x)))  \n",
    "        \n",
    "        # Second Conv2D + GELU + BatchNorm\n",
    "        x = self.gelu2(self.bn2(self.conv2(x)))  \n",
    "        \n",
    "        # Third Conv2D (Stride=2 for downsampling) + GELU + BatchNorm\n",
    "        x = self.gelu3(self.bn3(self.conv3(x)))  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=24):\n",
    "        super(CNNLayer, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.gelu1(self.bn1(self.conv1(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalInformationLayer(nn.Module):\n",
    "    def __init__(self, in_channels=24):\n",
    "        super(LocalInformationLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMHSA(nn.Module):\n",
    "    def __init__(self, dim, heads=8, kernel_size=3, stride=2, bias=True):\n",
    "        super(SMHSA, self).__init__()\n",
    "\n",
    "        assert dim % heads == 0, \"dim must be divisible by heads\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5  # Scaling factor for attention\n",
    "\n",
    "        # Linear layers to compute Query, Key, and Value\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "\n",
    "        # Depth-wise convolution for Q and K downsampling\n",
    "        self.spatial_reduction = nn.Conv2d(\n",
    "            dim, dim, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, \n",
    "            groups=dim, bias=False\n",
    "        )\n",
    "\n",
    "        # Initialize bias with proper shape for attention\n",
    "        self.bias = nn.Parameter(torch.zeros(1, heads, 1, 1)) if bias else None\n",
    "\n",
    "        # Final projection after self-attention\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Reshape input to sequence format\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)  # (B, seq_len, C)\n",
    "\n",
    "        # Compute Query, Key, and Value\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, -1, self.heads, C // self.heads).permute(0, 2, 1, 3)\n",
    "        k = k.view(B, -1, self.heads, C // self.heads).permute(0, 2, 1, 3)\n",
    "        v = v.view(B, -1, self.heads, C // self.heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        # ðŸ”¹ **Fix: Ensure correct reshaping before Conv2D**\n",
    "        q = q.permute(0, 1, 3, 2).reshape(B, C, H, W)\n",
    "        k = k.permute(0, 1, 3, 2).reshape(B, C, H, W)\n",
    "\n",
    "        # Apply spatial reduction\n",
    "        q = self.spatial_reduction(q)\n",
    "        k = self.spatial_reduction(k)\n",
    "\n",
    "        # Get reduced spatial dimensions after Conv2D\n",
    "        H_reduced, W_reduced = q.shape[2], q.shape[3]\n",
    "\n",
    "        # Reshape back for attention\n",
    "        q = q.reshape(B, self.heads, C // self.heads, H_reduced * W_reduced).permute(0, 1, 3, 2)\n",
    "        k = k.reshape(B, self.heads, C // self.heads, H_reduced * W_reduced).permute(0, 1, 3, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # Apply bias if enabled\n",
    "        if self.bias is not None:\n",
    "            # Expand bias to match attention dimensions\n",
    "            bias_expanded = self.bias.expand(B, self.heads, H_reduced * W_reduced, H_reduced * W_reduced)\n",
    "            attn = attn + bias_expanded\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # Apply attention to Value\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        # Reshape output back to spatial format\n",
    "        out = out.permute(0, 2, 1, 3).reshape(B, H_reduced * W_reduced, C)\n",
    "        out = self.to_out(out)\n",
    "        out = out.reshape(B, H_reduced, W_reduced, C).permute(0, 3, 1, 2)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFFN(nn.Module):\n",
    "    def __init__(self, dim, expansion=4):\n",
    "        super(RFFN, self).__init__()\n",
    "        \n",
    "        hidden_dim = dim * expansion\n",
    "\n",
    "        self.conv1 = nn.Conv2d(dim, hidden_dim, kernel_size=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_dim)\n",
    "        self.gelu1 = nn.GELU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_dim)\n",
    "        self.gelu2 = nn.GELU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(hidden_dim, dim, kernel_size=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "        x = self.gelu1(self.bn1(self.conv1(x)))\n",
    "        x = self.gelu2(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "\n",
    "        return x + residual\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FC(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        \"\"\"\n",
    "        Fully Connected (FC) Layer for Classification\n",
    "\n",
    "        Args:\n",
    "        - in_features: Number of input features (from GAP output, i.e., number of channels)\n",
    "        \"\"\"\n",
    "        super(FC, self).__init__()\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, 512)  # First Fully Connected Layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout to prevent overfitting\n",
    "        self.fc2 = nn.Linear(512, 2)  # Output Layer (Binary Classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)  # Apply Global Average Pooling\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten from (B, C, 1, 1) -> (B, C)\n",
    "        x = self.relu1(self.fc1(x))  # First Fully Connected Layer + ReLU\n",
    "        x = self.dropout(x)  # Apply Dropout\n",
    "        x = self.fc2(x)  # Final Output Layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TGCNN, self).__init__()\n",
    "\n",
    "        # CNN Layers\n",
    "        self.cnn_unit = CNNUnit(in_channels=22, out_channels=16)\n",
    "        self.cnn_layer1 = CNNLayer(in_channels=16, out_channels=24)\n",
    "        self.cnn_layer2 = CNNLayer(in_channels=24, out_channels=32)\n",
    "\n",
    "        # Transformer Block 1\n",
    "        self.LIL1 = LocalInformationLayer(in_channels=24)\n",
    "        self.norm1a = nn.LayerNorm([24, 2, 320])\n",
    "\n",
    "        self.multiheaded_att1 = SMHSA(dim=24, stride=1)  # Adjust stride if needed\n",
    "        self.norm1b = nn.LayerNorm([24, 2, 320])\n",
    "        self.recurrent1 = RFFN(dim=24)\n",
    "\n",
    "        # Transformer Block 2\n",
    "        self.LIL2 = LocalInformationLayer(in_channels=32)\n",
    "        self.norm1b = nn.LayerNorm([32, 1, 160])\n",
    "        self.multiheaded_att2 = SMHSA(dim=32, stride=1)  # Adjust stride if needed\n",
    "        self.norm1b = nn.LayerNorm([32, 1, 160])\n",
    "        self.recurrent2 = RFFN(dim=32)\n",
    "\n",
    "        # Global Average Pooling + Fully Connected Layer\n",
    "        self.fc = FC(in_features=32)  # Matches last CNN layer's output channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.cnn_unit(x)      \n",
    "        x = self.cnn_layer1(x)    \n",
    "        x = self.LIL1(x)          \n",
    "        x = self.multiheaded_att1(x)\n",
    "        x = self.recurrent1(x)    \n",
    "        x = self.cnn_layer2(x)    \n",
    "        x = self.LIL2(x)          \n",
    "        x = self.multiheaded_att2(x)\n",
    "        x = self.recurrent2(x)    \n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2])\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand(100,28,5,1280)\n",
    "model = TGCNN()\n",
    "output = model(data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[1], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mtorch\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mdata\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m DataLoader, TensorDataset\n\u001b[1;32m      5\u001b[0m \u001b[39mimport\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnumpy\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mas\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mnp\u001b[39;00m\n\u001b[0;32m----> 6\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39msklearn\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39mmodel_selection\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m train_test_split\n\u001b[1;32m      8\u001b[0m \u001b[39m# Check for GPU\u001b[39;00m\n\u001b[1;32m      9\u001b[0m device \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mdevice(\u001b[39m\"\u001b[39m\u001b[39mcuda\u001b[39m\u001b[39m\"\u001b[39m \u001b[39mif\u001b[39;00m torch\u001b[39m.\u001b[39mcuda\u001b[39m.\u001b[39mis_available() \u001b[39melse\u001b[39;00m \u001b[39m\"\u001b[39m\u001b[39mcpu\u001b[39m\u001b[39m\"\u001b[39m)\n",
      "File \u001b[0;32m~/Code/TGCNN/venv/lib/python3.11/site-packages/sklearn/__init__.py:74\u001b[0m\n\u001b[1;32m     69\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m (  \u001b[39m# noqa: F401 E402\u001b[39;00m\n\u001b[1;32m     70\u001b[0m     __check_build,\n\u001b[1;32m     71\u001b[0m     _distributor_init,\n\u001b[1;32m     72\u001b[0m )\n\u001b[1;32m     73\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mbase\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m clone  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[0;32m---> 74\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39mutils\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m_show_versions\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m show_versions  \u001b[39m# noqa: E402\u001b[39;00m\n\u001b[1;32m     76\u001b[0m _submodules \u001b[39m=\u001b[39m [\n\u001b[1;32m     77\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcalibration\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m     78\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcluster\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    114\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mcompose\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    115\u001b[0m ]\n\u001b[1;32m    117\u001b[0m __all__ \u001b[39m=\u001b[39m _submodules \u001b[39m+\u001b[39m [\n\u001b[1;32m    118\u001b[0m     \u001b[39m# Non-modules:\u001b[39;00m\n\u001b[1;32m    119\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mclone\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    123\u001b[0m     \u001b[39m\"\u001b[39m\u001b[39mshow_versions\u001b[39m\u001b[39m\"\u001b[39m,\n\u001b[1;32m    124\u001b[0m ]\n",
      "File \u001b[0;32m~/Code/TGCNN/venv/lib/python3.11/site-packages/sklearn/utils/_show_versions.py:16\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mthreadpoolctl\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m threadpool_info\n\u001b[1;32m     15\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39m.\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m __version__\n\u001b[0;32m---> 16\u001b[0m \u001b[39mfrom\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m.\u001b[39;00m\u001b[39m_openmp_helpers\u001b[39;00m\u001b[39m \u001b[39m\u001b[39mimport\u001b[39;00m _openmp_parallelism_enabled\n\u001b[1;32m     19\u001b[0m \u001b[39mdef\u001b[39;00m\u001b[39m \u001b[39m\u001b[39m_get_sys_info\u001b[39m():\n\u001b[1;32m     20\u001b[0m \u001b[39m    \u001b[39m\u001b[39m\"\"\"System information\u001b[39;00m\n\u001b[1;32m     21\u001b[0m \n\u001b[1;32m     22\u001b[0m \u001b[39m    Returns\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     26\u001b[0m \n\u001b[1;32m     27\u001b[0m \u001b[39m    \"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m<frozen importlib._bootstrap>:405\u001b[0m, in \u001b[0;36mparent\u001b[0;34m(self)\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Example Data (Ensure your real data is loaded correctly)\n",
    "spectrogram_data = np.random.rand(1000, 28, 5, 1280)  # Simulated input\n",
    "y = np.random.randint(0, 2, (1000,))  # Simulated binary classification labels\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectrogram_data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, Loss Function, and Optimizer\n",
    "model = TGCNN().to(device)  # Move model to GPU if available\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Parameters\n",
    "num_epochs = 20  # Adjust based on dataset size\n",
    "print_interval = 5  # Print every 5 batches\n",
    "\n",
    "# ðŸš€ Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        \n",
    "        loss = criterion(outputs, targets)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # Optional: Gradient clipping\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        if batch_idx % print_interval == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"ðŸ”¥ Epoch [{epoch+1}/{num_epochs}] -> Loss: {total_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"TGCNN_model.pth\")\n",
    "print(\"âœ… Model saved successfully!\")\n",
    "\n",
    "# ðŸš€ Evaluation Function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"ðŸŽ¯ Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Run Evaluation\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Example Data (Ensure your real data is loaded correctly)\n",
    "spectrogram_data = np.random.rand(1000, 28, 5, 1280)  # Simulated input\n",
    "y = np.random.randint(0, 2, (1000,))  # Simulated binary classification labels\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectrogram_data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, Loss Function, and Optimizer\n",
    "model = TGCNN().to(device)  # Move model to GPU if available\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Parameters\n",
    "num_epochs = 20  # Adjust based on dataset size\n",
    "print_interval = 5  # Print every 5 batches\n",
    "\n",
    "# ðŸš€ Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        \n",
    "        loss = criterion(outputs, targets)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # Optional: Gradient clipping\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        if batch_idx % print_interval == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"ðŸ”¥ Epoch [{epoch+1}/{num_epochs}] -> Loss: {total_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"TGCNN_model.pth\")\n",
    "print(\"âœ… Model saved successfully!\")\n",
    "\n",
    "# ðŸš€ Evaluation Function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"ðŸŽ¯ Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Run Evaluation\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
