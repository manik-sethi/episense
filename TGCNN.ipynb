{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aws_utils import s3_client, bucket_name, fs\n",
    "import glob, os, io, pickle\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Dataset, WeightedRandomSampler"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: torch.Size([14464, 22, 1471]) torch.Size([14464])\n"
     ]
    }
   ],
   "source": [
    "key = \"final_data.pkl\"                           # object lives at s3://maniks-chb-mit/final_data.pkl\n",
    "\n",
    "buf = io.BytesIO()                              # inâ€‘memory buffer\n",
    "s3_client.download_fileobj(bucket_name, key, buf)\n",
    "buf.seek(0)                                     # rewind\n",
    "\n",
    "data = pickle.load(buf)                         # unâ€‘pickle\n",
    "X, y = data[\"X\"], data[\"y\"]\n",
    "\n",
    "print(\"Loaded:\", X.shape, y.shape)               # e.g.  torch.Size([1601, 22, 1471])  (1601,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "after balancing: tensor([3231, 3231])\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "\n",
    "# -------------------------------------------------------------\n",
    "# X : Tensor of shape (N, â€¦)          â€“ your spectrograms / epochs\n",
    "# y : Tensor of shape (N,)            â€“ 0 = interâ€‘ictal, 1 = preâ€‘ictal\n",
    "# -------------------------------------------------------------\n",
    "torch.manual_seed(42)                 # â† for reproducibility; optional\n",
    "\n",
    "# 1) find indices of each class\n",
    "idx_int = (y == 0).nonzero(as_tuple=True)[0]   # interâ€‘ictal indices\n",
    "idx_pre = (y == 1).nonzero(as_tuple=True)[0]   # preâ€‘ictal indices\n",
    "\n",
    "# 2) decide how many interâ€‘ictal samples to keep\n",
    "#    here: keep as many interâ€‘ictal as preâ€‘ictal â‡’ perfectly balanced\n",
    "k = len(idx_pre)\n",
    "\n",
    "perm      = torch.randperm(len(idx_int))       # shuffle\n",
    "idx_int_k = idx_int[perm[:k]]                  # first k indices\n",
    "\n",
    "# 3) combine & shuffle (optional but recommended)\n",
    "idx_keep = torch.cat([idx_pre, idx_int_k])\n",
    "idx_keep = idx_keep[torch.randperm(len(idx_keep))]\n",
    "\n",
    "# 4) slice tensors\n",
    "X_bal = X[idx_keep]\n",
    "y_bal = y[idx_keep]\n",
    "\n",
    "print(\"after balancing:\", torch.bincount(y_bal))\n",
    "# tensor([k, k])           â† equal counts now\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = TensorDataset(X_bal, y_bal)\n",
    "class_sample_count = torch.bincount(y_bal)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([3231, 3231])\n"
     ]
    }
   ],
   "source": [
    "print(class_sample_count)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=16):\n",
    "        super(CNNUnit, self).__init__()\n",
    "        \n",
    "        # First 3x3 Conv Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        \n",
    "        # Second 3x3 Conv Layer \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu2 = nn.GELU()\n",
    "\n",
    "        # Third 3x3 Conv Layer with Stride 2 (Downsampling)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu3 = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # First Conv2D + GELU + BatchNorm\n",
    "        x = self.gelu1(self.bn1(self.conv1(x)))  \n",
    "        \n",
    "        # Second Conv2D + GELU + BatchNorm\n",
    "        x = self.gelu2(self.bn2(self.conv2(x)))  \n",
    "        \n",
    "        # Third Conv2D (Stride=2 for downsampling) + GELU + BatchNorm\n",
    "        x = self.gelu3(self.bn3(self.conv3(x)))  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=24):\n",
    "        super(CNNLayer, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.gelu1(self.bn1(self.conv1(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalInformationLayer(nn.Module):\n",
    "    def __init__(self, in_channels=24):\n",
    "        super(LocalInformationLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMHSA(nn.Module):\n",
    "    def __init__(self, dim, heads=8, kernel_size=3, stride=2, bias=True):\n",
    "        super(SMHSA, self).__init__()\n",
    "\n",
    "        assert dim % heads == 0, \"dim must be divisible by heads\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5  # Scaling factor for attention\n",
    "\n",
    "        # Linear layers to compute Query, Key, and Value\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "\n",
    "        # Depth-wise convolution for Q and K downsampling\n",
    "        self.spatial_reduction = nn.Conv2d(\n",
    "            dim, dim, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, \n",
    "            groups=dim, bias=False\n",
    "        )\n",
    "\n",
    "        # Initialize bias with proper shape for attention\n",
    "        self.bias = nn.Parameter(torch.zeros(1, heads, 1, 1)) if bias else None\n",
    "\n",
    "        # Final projection after self-attention\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Reshape input to sequence format\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)  # (B, seq_len, C)\n",
    "\n",
    "        # Compute Query, Key, and Value\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, -1, self.heads, C // self.heads).permute(0, 2, 1, 3)\n",
    "        k = k.view(B, -1, self.heads, C // self.heads).permute(0, 2, 1, 3)\n",
    "        v = v.view(B, -1, self.heads, C // self.heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        # ðŸ”¹ **Fix: Ensure correct reshaping before Conv2D**\n",
    "        q = q.permute(0, 1, 3, 2).reshape(B, C, H, W)\n",
    "        k = k.permute(0, 1, 3, 2).reshape(B, C, H, W)\n",
    "\n",
    "        # Apply spatial reduction\n",
    "        q = self.spatial_reduction(q)\n",
    "        k = self.spatial_reduction(k)\n",
    "\n",
    "        # Get reduced spatial dimensions after Conv2D\n",
    "        H_reduced, W_reduced = q.shape[2], q.shape[3]\n",
    "\n",
    "        # Reshape back for attention\n",
    "        q = q.reshape(B, self.heads, C // self.heads, H_reduced * W_reduced).permute(0, 1, 3, 2)\n",
    "        k = k.reshape(B, self.heads, C // self.heads, H_reduced * W_reduced).permute(0, 1, 3, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # Apply bias if enabled\n",
    "        if self.bias is not None:\n",
    "            # Expand bias to match attention dimensions\n",
    "            bias_expanded = self.bias.expand(B, self.heads, H_reduced * W_reduced, H_reduced * W_reduced)\n",
    "            attn = attn + bias_expanded\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # Apply attention to Value\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        # Reshape output back to spatial format\n",
    "        out = out.permute(0, 2, 1, 3).reshape(B, H_reduced * W_reduced, C)\n",
    "        out = self.to_out(out)\n",
    "        out = out.reshape(B, H_reduced, W_reduced, C).permute(0, 3, 1, 2)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFFN(nn.Module):\n",
    "    def __init__(self, dim, expansion=4):\n",
    "        super(RFFN, self).__init__()\n",
    "        \n",
    "        hidden_dim = dim * expansion\n",
    "\n",
    "        self.conv1 = nn.Conv2d(dim, hidden_dim, kernel_size=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_dim)\n",
    "        self.gelu1 = nn.GELU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_dim)\n",
    "        self.gelu2 = nn.GELU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(hidden_dim, dim, kernel_size=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "        x = self.gelu1(self.bn1(self.conv1(x)))\n",
    "        x = self.gelu2(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "\n",
    "        return x + residual\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FC(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        \"\"\"\n",
    "        Fully Connected (FC) Layer for Classification\n",
    "\n",
    "        Args:\n",
    "        - in_features: Number of input features (from GAP output, i.e., number of channels)\n",
    "        \"\"\"\n",
    "        super(FC, self).__init__()\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, 512)  # First Fully Connected Layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout to prevent overfitting\n",
    "        self.fc2 = nn.Linear(512, 2)  # Output Layer (Binary Classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)  # Apply Global Average Pooling\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten from (B, C, 1, 1) -> (B, C)\n",
    "        x = self.relu1(self.fc1(x))  # First Fully Connected Layer + ReLU\n",
    "        x = self.dropout(x)  # Apply Dropout\n",
    "        x = self.fc2(x)  # Final Output Layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TGCNN, self).__init__()\n",
    "\n",
    "        # CNN Layers\n",
    "        self.cnn_unit = CNNUnit(in_channels=22, out_channels=16)\n",
    "        self.cnn_layer1 = CNNLayer(in_channels=16, out_channels=24)\n",
    "        self.cnn_layer2 = CNNLayer(in_channels=24, out_channels=32)\n",
    "\n",
    "        # Transformer Block 1\n",
    "        self.LIL1 = LocalInformationLayer(in_channels=24)\n",
    "        self.norm1a = nn.LayerNorm([24, 2, 320])\n",
    "\n",
    "        self.multiheaded_att1 = SMHSA(dim=24, stride=1)  # Adjust stride if needed\n",
    "        self.norm1b = nn.LayerNorm([24, 2, 320])\n",
    "        self.recurrent1 = RFFN(dim=24)\n",
    "\n",
    "        # Transformer Block 2\n",
    "        self.LIL2 = LocalInformationLayer(in_channels=32)\n",
    "        self.norm1b = nn.LayerNorm([32, 1, 160])\n",
    "        self.multiheaded_att2 = SMHSA(dim=32, stride=1)  # Adjust stride if needed\n",
    "        self.norm1b = nn.LayerNorm([32, 1, 160])\n",
    "        self.recurrent2 = RFFN(dim=32)\n",
    "\n",
    "        # Global Average Pooling + Fully Connected Layer\n",
    "        self.fc = FC(in_features=32)  # Matches last CNN layer's output channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)\n",
    "        x = self.cnn_unit(x)      \n",
    "        x = self.cnn_layer1(x)    \n",
    "        x = self.LIL1(x)          \n",
    "        x = self.multiheaded_att1(x)\n",
    "        x = self.recurrent1(x)    \n",
    "        x = self.cnn_layer2(x)    \n",
    "        x = self.LIL2(x)          \n",
    "        x = self.multiheaded_att2(x)\n",
    "        x = self.recurrent2(x)    \n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2])\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand(100,22,1280)\n",
    "model = TGCNN()\n",
    "output = model(data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/20], Step [1/162], Loss: 0.7061\n",
      "Epoch [1/20], Step [6/162], Loss: 0.6934\n",
      "Epoch [1/20], Step [11/162], Loss: 0.6549\n",
      "Epoch [1/20], Step [16/162], Loss: 0.7899\n",
      "Epoch [1/20], Step [21/162], Loss: 0.6281\n",
      "Epoch [1/20], Step [26/162], Loss: 0.5240\n",
      "Epoch [1/20], Step [31/162], Loss: 0.4939\n",
      "Epoch [1/20], Step [36/162], Loss: 0.5736\n",
      "Epoch [1/20], Step [41/162], Loss: 0.7252\n",
      "Epoch [1/20], Step [46/162], Loss: 0.7063\n",
      "Epoch [1/20], Step [51/162], Loss: 0.4075\n",
      "Epoch [1/20], Step [56/162], Loss: 0.5038\n",
      "Epoch [1/20], Step [61/162], Loss: 0.6376\n",
      "Epoch [1/20], Step [66/162], Loss: 0.4342\n",
      "Epoch [1/20], Step [71/162], Loss: 0.6965\n",
      "Epoch [1/20], Step [76/162], Loss: 0.4697\n",
      "Epoch [1/20], Step [81/162], Loss: 0.6698\n",
      "Epoch [1/20], Step [86/162], Loss: 0.4742\n",
      "Epoch [1/20], Step [91/162], Loss: 0.8918\n",
      "Epoch [1/20], Step [96/162], Loss: 0.5903\n",
      "Epoch [1/20], Step [101/162], Loss: 0.4362\n",
      "Epoch [1/20], Step [106/162], Loss: 0.4012\n",
      "Epoch [1/20], Step [111/162], Loss: 0.4998\n",
      "Epoch [1/20], Step [116/162], Loss: 0.3235\n",
      "Epoch [1/20], Step [121/162], Loss: 0.3912\n",
      "Epoch [1/20], Step [126/162], Loss: 0.2159\n",
      "Epoch [1/20], Step [131/162], Loss: 0.3754\n",
      "Epoch [1/20], Step [136/162], Loss: 0.4220\n",
      "Epoch [1/20], Step [141/162], Loss: 0.3790\n",
      "Epoch [1/20], Step [146/162], Loss: 0.3553\n",
      "Epoch [1/20], Step [151/162], Loss: 0.5884\n",
      "Epoch [1/20], Step [156/162], Loss: 0.4193\n",
      "Epoch [1/20], Step [161/162], Loss: 0.5605\n",
      "ðŸ”¥ Epoch [1/20] -> Loss: 0.5182, Accuracy: 74.97%\n",
      "Epoch [2/20], Step [1/162], Loss: 0.5766\n",
      "Epoch [2/20], Step [6/162], Loss: 0.5230\n",
      "Epoch [2/20], Step [11/162], Loss: 0.3726\n",
      "Epoch [2/20], Step [16/162], Loss: 0.3517\n",
      "Epoch [2/20], Step [21/162], Loss: 0.6304\n",
      "Epoch [2/20], Step [26/162], Loss: 0.5061\n",
      "Epoch [2/20], Step [31/162], Loss: 0.3094\n",
      "Epoch [2/20], Step [36/162], Loss: 0.3661\n",
      "Epoch [2/20], Step [41/162], Loss: 0.3726\n",
      "Epoch [2/20], Step [46/162], Loss: 0.2036\n",
      "Epoch [2/20], Step [51/162], Loss: 0.3739\n",
      "Epoch [2/20], Step [56/162], Loss: 0.2328\n",
      "Epoch [2/20], Step [61/162], Loss: 0.4260\n",
      "Epoch [2/20], Step [66/162], Loss: 0.2642\n",
      "Epoch [2/20], Step [71/162], Loss: 0.2637\n",
      "Epoch [2/20], Step [76/162], Loss: 0.3719\n",
      "Epoch [2/20], Step [81/162], Loss: 0.5108\n",
      "Epoch [2/20], Step [86/162], Loss: 0.3775\n",
      "Epoch [2/20], Step [91/162], Loss: 0.2417\n",
      "Epoch [2/20], Step [96/162], Loss: 0.4405\n",
      "Epoch [2/20], Step [101/162], Loss: 0.2987\n",
      "Epoch [2/20], Step [106/162], Loss: 0.5077\n",
      "Epoch [2/20], Step [111/162], Loss: 0.1502\n",
      "Epoch [2/20], Step [116/162], Loss: 0.4443\n",
      "Epoch [2/20], Step [121/162], Loss: 0.3876\n",
      "Epoch [2/20], Step [126/162], Loss: 0.2854\n",
      "Epoch [2/20], Step [131/162], Loss: 0.2442\n",
      "Epoch [2/20], Step [136/162], Loss: 0.2650\n",
      "Epoch [2/20], Step [141/162], Loss: 0.5112\n",
      "Epoch [2/20], Step [146/162], Loss: 0.3853\n",
      "Epoch [2/20], Step [151/162], Loss: 0.1688\n",
      "Epoch [2/20], Step [156/162], Loss: 0.2879\n",
      "Epoch [2/20], Step [161/162], Loss: 0.4899\n",
      "ðŸ”¥ Epoch [2/20] -> Loss: 0.3670, Accuracy: 84.58%\n",
      "Epoch [3/20], Step [1/162], Loss: 0.2720\n",
      "Epoch [3/20], Step [6/162], Loss: 0.2499\n",
      "Epoch [3/20], Step [11/162], Loss: 0.4560\n",
      "Epoch [3/20], Step [16/162], Loss: 0.3276\n",
      "Epoch [3/20], Step [21/162], Loss: 0.4195\n",
      "Epoch [3/20], Step [26/162], Loss: 0.3238\n",
      "Epoch [3/20], Step [31/162], Loss: 0.2197\n",
      "Epoch [3/20], Step [36/162], Loss: 0.2924\n",
      "Epoch [3/20], Step [41/162], Loss: 0.1798\n",
      "Epoch [3/20], Step [46/162], Loss: 0.1929\n",
      "Epoch [3/20], Step [51/162], Loss: 0.2599\n",
      "Epoch [3/20], Step [56/162], Loss: 0.2808\n",
      "Epoch [3/20], Step [61/162], Loss: 0.1991\n",
      "Epoch [3/20], Step [66/162], Loss: 0.2714\n",
      "Epoch [3/20], Step [71/162], Loss: 0.2157\n",
      "Epoch [3/20], Step [76/162], Loss: 0.2400\n",
      "Epoch [3/20], Step [81/162], Loss: 0.2532\n",
      "Epoch [3/20], Step [86/162], Loss: 0.7296\n",
      "Epoch [3/20], Step [91/162], Loss: 0.2091\n",
      "Epoch [3/20], Step [96/162], Loss: 0.3217\n",
      "Epoch [3/20], Step [101/162], Loss: 0.2262\n",
      "Epoch [3/20], Step [106/162], Loss: 0.5103\n",
      "Epoch [3/20], Step [111/162], Loss: 0.2188\n",
      "Epoch [3/20], Step [116/162], Loss: 0.3269\n",
      "Epoch [3/20], Step [121/162], Loss: 0.4376\n",
      "Epoch [3/20], Step [126/162], Loss: 0.1972\n",
      "Epoch [3/20], Step [131/162], Loss: 0.2373\n",
      "Epoch [3/20], Step [136/162], Loss: 0.2874\n",
      "Epoch [3/20], Step [141/162], Loss: 0.1725\n",
      "Epoch [3/20], Step [146/162], Loss: 0.2862\n",
      "Epoch [3/20], Step [151/162], Loss: 0.1861\n",
      "Epoch [3/20], Step [156/162], Loss: 0.1400\n",
      "Epoch [3/20], Step [161/162], Loss: 0.1676\n",
      "ðŸ”¥ Epoch [3/20] -> Loss: 0.3131, Accuracy: 87.02%\n",
      "Epoch [4/20], Step [1/162], Loss: 0.3891\n",
      "Epoch [4/20], Step [6/162], Loss: 0.2408\n",
      "Epoch [4/20], Step [11/162], Loss: 0.3456\n",
      "Epoch [4/20], Step [16/162], Loss: 0.2146\n",
      "Epoch [4/20], Step [21/162], Loss: 0.2121\n",
      "Epoch [4/20], Step [26/162], Loss: 0.6533\n",
      "Epoch [4/20], Step [31/162], Loss: 0.2422\n",
      "Epoch [4/20], Step [36/162], Loss: 0.1838\n",
      "Epoch [4/20], Step [41/162], Loss: 0.2556\n",
      "Epoch [4/20], Step [46/162], Loss: 0.2594\n",
      "Epoch [4/20], Step [51/162], Loss: 0.2783\n",
      "Epoch [4/20], Step [56/162], Loss: 0.3921\n",
      "Epoch [4/20], Step [61/162], Loss: 0.2332\n",
      "Epoch [4/20], Step [66/162], Loss: 0.2529\n",
      "Epoch [4/20], Step [71/162], Loss: 0.1635\n",
      "Epoch [4/20], Step [76/162], Loss: 0.2406\n",
      "Epoch [4/20], Step [81/162], Loss: 0.2511\n",
      "Epoch [4/20], Step [86/162], Loss: 0.3622\n",
      "Epoch [4/20], Step [91/162], Loss: 0.3317\n",
      "Epoch [4/20], Step [96/162], Loss: 0.2486\n",
      "Epoch [4/20], Step [101/162], Loss: 0.1861\n",
      "Epoch [4/20], Step [106/162], Loss: 0.5543\n",
      "Epoch [4/20], Step [111/162], Loss: 0.2666\n",
      "Epoch [4/20], Step [116/162], Loss: 0.2116\n",
      "Epoch [4/20], Step [121/162], Loss: 0.5592\n",
      "Epoch [4/20], Step [126/162], Loss: 0.2338\n",
      "Epoch [4/20], Step [131/162], Loss: 0.1328\n",
      "Epoch [4/20], Step [136/162], Loss: 0.3775\n",
      "Epoch [4/20], Step [141/162], Loss: 0.1348\n",
      "Epoch [4/20], Step [146/162], Loss: 0.3362\n",
      "Epoch [4/20], Step [151/162], Loss: 0.3173\n",
      "Epoch [4/20], Step [156/162], Loss: 0.1947\n",
      "Epoch [4/20], Step [161/162], Loss: 0.1442\n",
      "ðŸ”¥ Epoch [4/20] -> Loss: 0.3035, Accuracy: 87.68%\n",
      "Epoch [5/20], Step [1/162], Loss: 0.2299\n",
      "Epoch [5/20], Step [6/162], Loss: 0.1916\n",
      "Epoch [5/20], Step [11/162], Loss: 0.3684\n",
      "Epoch [5/20], Step [16/162], Loss: 0.4206\n",
      "Epoch [5/20], Step [21/162], Loss: 0.4626\n",
      "Epoch [5/20], Step [26/162], Loss: 0.3504\n",
      "Epoch [5/20], Step [31/162], Loss: 0.3096\n",
      "Epoch [5/20], Step [36/162], Loss: 0.1654\n",
      "Epoch [5/20], Step [41/162], Loss: 0.2515\n",
      "Epoch [5/20], Step [46/162], Loss: 0.3054\n",
      "Epoch [5/20], Step [51/162], Loss: 0.1734\n",
      "Epoch [5/20], Step [56/162], Loss: 0.3053\n",
      "Epoch [5/20], Step [61/162], Loss: 0.0823\n",
      "Epoch [5/20], Step [66/162], Loss: 0.6221\n",
      "Epoch [5/20], Step [71/162], Loss: 0.2267\n",
      "Epoch [5/20], Step [76/162], Loss: 0.1485\n",
      "Epoch [5/20], Step [81/162], Loss: 0.5456\n",
      "Epoch [5/20], Step [86/162], Loss: 0.1555\n",
      "Epoch [5/20], Step [91/162], Loss: 0.2934\n",
      "Epoch [5/20], Step [96/162], Loss: 0.1637\n",
      "Epoch [5/20], Step [101/162], Loss: 0.2428\n",
      "Epoch [5/20], Step [106/162], Loss: 0.1625\n",
      "Epoch [5/20], Step [111/162], Loss: 0.2767\n",
      "Epoch [5/20], Step [116/162], Loss: 0.1691\n",
      "Epoch [5/20], Step [121/162], Loss: 0.4295\n",
      "Epoch [5/20], Step [126/162], Loss: 0.1624\n",
      "Epoch [5/20], Step [131/162], Loss: 0.2543\n",
      "Epoch [5/20], Step [136/162], Loss: 0.2856\n",
      "Epoch [5/20], Step [141/162], Loss: 0.2593\n",
      "Epoch [5/20], Step [146/162], Loss: 0.1806\n",
      "Epoch [5/20], Step [151/162], Loss: 0.1790\n",
      "Epoch [5/20], Step [156/162], Loss: 0.2316\n",
      "Epoch [5/20], Step [161/162], Loss: 0.2836\n",
      "ðŸ”¥ Epoch [5/20] -> Loss: 0.2767, Accuracy: 89.05%\n",
      "Epoch [6/20], Step [1/162], Loss: 0.2024\n",
      "Epoch [6/20], Step [6/162], Loss: 0.3633\n",
      "Epoch [6/20], Step [11/162], Loss: 0.2048\n",
      "Epoch [6/20], Step [16/162], Loss: 0.2102\n",
      "Epoch [6/20], Step [21/162], Loss: 0.6297\n",
      "Epoch [6/20], Step [26/162], Loss: 0.0821\n",
      "Epoch [6/20], Step [31/162], Loss: 0.4816\n",
      "Epoch [6/20], Step [36/162], Loss: 0.4043\n",
      "Epoch [6/20], Step [41/162], Loss: 0.2007\n",
      "Epoch [6/20], Step [46/162], Loss: 0.1796\n",
      "Epoch [6/20], Step [51/162], Loss: 0.1203\n",
      "Epoch [6/20], Step [56/162], Loss: 0.1246\n",
      "Epoch [6/20], Step [61/162], Loss: 0.5419\n",
      "Epoch [6/20], Step [66/162], Loss: 0.1904\n",
      "Epoch [6/20], Step [71/162], Loss: 0.1979\n",
      "Epoch [6/20], Step [76/162], Loss: 0.2465\n",
      "Epoch [6/20], Step [81/162], Loss: 0.1842\n",
      "Epoch [6/20], Step [86/162], Loss: 0.1692\n",
      "Epoch [6/20], Step [91/162], Loss: 0.3281\n",
      "Epoch [6/20], Step [96/162], Loss: 0.2323\n",
      "Epoch [6/20], Step [101/162], Loss: 0.2301\n",
      "Epoch [6/20], Step [106/162], Loss: 0.2281\n",
      "Epoch [6/20], Step [111/162], Loss: 0.1827\n",
      "Epoch [6/20], Step [116/162], Loss: 0.3157\n",
      "Epoch [6/20], Step [121/162], Loss: 0.5925\n",
      "Epoch [6/20], Step [126/162], Loss: 0.3686\n",
      "Epoch [6/20], Step [131/162], Loss: 0.2980\n",
      "Epoch [6/20], Step [136/162], Loss: 0.2247\n",
      "Epoch [6/20], Step [141/162], Loss: 0.1200\n",
      "Epoch [6/20], Step [146/162], Loss: 0.2946\n",
      "Epoch [6/20], Step [151/162], Loss: 0.2615\n",
      "Epoch [6/20], Step [156/162], Loss: 0.1625\n",
      "Epoch [6/20], Step [161/162], Loss: 0.1529\n",
      "ðŸ”¥ Epoch [6/20] -> Loss: 0.2708, Accuracy: 89.50%\n",
      "Epoch [7/20], Step [1/162], Loss: 0.2281\n",
      "Epoch [7/20], Step [6/162], Loss: 0.1379\n",
      "Epoch [7/20], Step [11/162], Loss: 0.1064\n",
      "Epoch [7/20], Step [16/162], Loss: 0.3532\n",
      "Epoch [7/20], Step [21/162], Loss: 0.1896\n",
      "Epoch [7/20], Step [26/162], Loss: 0.2165\n",
      "Epoch [7/20], Step [31/162], Loss: 0.1328\n",
      "Epoch [7/20], Step [36/162], Loss: 0.3176\n",
      "Epoch [7/20], Step [41/162], Loss: 0.2553\n",
      "Epoch [7/20], Step [46/162], Loss: 0.2784\n",
      "Epoch [7/20], Step [51/162], Loss: 0.1951\n",
      "Epoch [7/20], Step [56/162], Loss: 0.1658\n",
      "Epoch [7/20], Step [61/162], Loss: 0.2426\n",
      "Epoch [7/20], Step [66/162], Loss: 0.1507\n",
      "Epoch [7/20], Step [71/162], Loss: 0.3826\n",
      "Epoch [7/20], Step [76/162], Loss: 0.1314\n",
      "Epoch [7/20], Step [81/162], Loss: 0.3088\n",
      "Epoch [7/20], Step [86/162], Loss: 0.1605\n",
      "Epoch [7/20], Step [91/162], Loss: 0.2828\n",
      "Epoch [7/20], Step [96/162], Loss: 0.2666\n",
      "Epoch [7/20], Step [101/162], Loss: 0.1873\n",
      "Epoch [7/20], Step [106/162], Loss: 0.3132\n",
      "Epoch [7/20], Step [111/162], Loss: 0.2483\n",
      "Epoch [7/20], Step [116/162], Loss: 0.2655\n",
      "Epoch [7/20], Step [121/162], Loss: 0.2024\n",
      "Epoch [7/20], Step [126/162], Loss: 0.2219\n",
      "Epoch [7/20], Step [131/162], Loss: 0.2449\n",
      "Epoch [7/20], Step [136/162], Loss: 0.0865\n",
      "Epoch [7/20], Step [141/162], Loss: 0.2478\n",
      "Epoch [7/20], Step [146/162], Loss: 0.1553\n",
      "Epoch [7/20], Step [151/162], Loss: 0.2670\n",
      "Epoch [7/20], Step [156/162], Loss: 0.2291\n",
      "Epoch [7/20], Step [161/162], Loss: 0.2448\n",
      "ðŸ”¥ Epoch [7/20] -> Loss: 0.2579, Accuracy: 89.92%\n",
      "Epoch [8/20], Step [1/162], Loss: 0.2052\n",
      "Epoch [8/20], Step [6/162], Loss: 0.2007\n",
      "Epoch [8/20], Step [11/162], Loss: 0.2765\n",
      "Epoch [8/20], Step [16/162], Loss: 0.4342\n",
      "Epoch [8/20], Step [21/162], Loss: 0.2418\n",
      "Epoch [8/20], Step [26/162], Loss: 0.2026\n",
      "Epoch [8/20], Step [31/162], Loss: 0.1554\n",
      "Epoch [8/20], Step [36/162], Loss: 0.2172\n",
      "Epoch [8/20], Step [41/162], Loss: 0.1976\n",
      "Epoch [8/20], Step [46/162], Loss: 0.2678\n",
      "Epoch [8/20], Step [51/162], Loss: 0.2946\n",
      "Epoch [8/20], Step [56/162], Loss: 0.1601\n",
      "Epoch [8/20], Step [61/162], Loss: 0.4719\n",
      "Epoch [8/20], Step [66/162], Loss: 0.2037\n",
      "Epoch [8/20], Step [71/162], Loss: 0.2763\n",
      "Epoch [8/20], Step [76/162], Loss: 0.2731\n",
      "Epoch [8/20], Step [81/162], Loss: 0.5257\n",
      "Epoch [8/20], Step [86/162], Loss: 0.1370\n",
      "Epoch [8/20], Step [91/162], Loss: 0.2133\n",
      "Epoch [8/20], Step [96/162], Loss: 0.3271\n",
      "Epoch [8/20], Step [101/162], Loss: 0.2030\n",
      "Epoch [8/20], Step [106/162], Loss: 0.0536\n",
      "Epoch [8/20], Step [111/162], Loss: 0.2104\n",
      "Epoch [8/20], Step [116/162], Loss: 0.0680\n",
      "Epoch [8/20], Step [121/162], Loss: 0.1198\n",
      "Epoch [8/20], Step [126/162], Loss: 0.2517\n",
      "Epoch [8/20], Step [131/162], Loss: 0.2672\n",
      "Epoch [8/20], Step [136/162], Loss: 0.2352\n",
      "Epoch [8/20], Step [141/162], Loss: 0.1834\n",
      "Epoch [8/20], Step [146/162], Loss: 0.5068\n",
      "Epoch [8/20], Step [151/162], Loss: 0.1454\n",
      "Epoch [8/20], Step [156/162], Loss: 0.2477\n",
      "Epoch [8/20], Step [161/162], Loss: 0.1199\n",
      "ðŸ”¥ Epoch [8/20] -> Loss: 0.2542, Accuracy: 89.96%\n",
      "Epoch [9/20], Step [1/162], Loss: 0.5941\n",
      "Epoch [9/20], Step [6/162], Loss: 0.3402\n",
      "Epoch [9/20], Step [11/162], Loss: 0.2723\n",
      "Epoch [9/20], Step [16/162], Loss: 0.1344\n",
      "Epoch [9/20], Step [21/162], Loss: 0.1488\n",
      "Epoch [9/20], Step [26/162], Loss: 0.1179\n",
      "Epoch [9/20], Step [31/162], Loss: 0.1102\n",
      "Epoch [9/20], Step [36/162], Loss: 0.1085\n",
      "Epoch [9/20], Step [41/162], Loss: 0.1942\n",
      "Epoch [9/20], Step [46/162], Loss: 0.2632\n",
      "Epoch [9/20], Step [51/162], Loss: 0.2713\n",
      "Epoch [9/20], Step [56/162], Loss: 0.3462\n",
      "Epoch [9/20], Step [61/162], Loss: 0.2222\n",
      "Epoch [9/20], Step [66/162], Loss: 0.2141\n",
      "Epoch [9/20], Step [71/162], Loss: 0.1291\n",
      "Epoch [9/20], Step [76/162], Loss: 0.3069\n",
      "Epoch [9/20], Step [81/162], Loss: 0.2508\n",
      "Epoch [9/20], Step [86/162], Loss: 0.2216\n",
      "Epoch [9/20], Step [91/162], Loss: 0.2309\n",
      "Epoch [9/20], Step [96/162], Loss: 0.3822\n",
      "Epoch [9/20], Step [101/162], Loss: 0.1544\n",
      "Epoch [9/20], Step [106/162], Loss: 0.2986\n",
      "Epoch [9/20], Step [111/162], Loss: 0.2416\n",
      "Epoch [9/20], Step [116/162], Loss: 0.2003\n",
      "Epoch [9/20], Step [121/162], Loss: 0.3125\n",
      "Epoch [9/20], Step [126/162], Loss: 0.0949\n",
      "Epoch [9/20], Step [131/162], Loss: 0.2041\n",
      "Epoch [9/20], Step [136/162], Loss: 0.1040\n",
      "Epoch [9/20], Step [141/162], Loss: 0.3220\n",
      "Epoch [9/20], Step [146/162], Loss: 0.2065\n",
      "Epoch [9/20], Step [151/162], Loss: 0.8619\n",
      "Epoch [9/20], Step [156/162], Loss: 0.3093\n",
      "Epoch [9/20], Step [161/162], Loss: 0.1812\n",
      "ðŸ”¥ Epoch [9/20] -> Loss: 0.2499, Accuracy: 90.38%\n",
      "Epoch [10/20], Step [1/162], Loss: 0.1512\n",
      "Epoch [10/20], Step [6/162], Loss: 0.1124\n",
      "Epoch [10/20], Step [11/162], Loss: 0.1391\n",
      "Epoch [10/20], Step [16/162], Loss: 0.3925\n",
      "Epoch [10/20], Step [21/162], Loss: 0.1872\n",
      "Epoch [10/20], Step [26/162], Loss: 0.2389\n",
      "Epoch [10/20], Step [31/162], Loss: 0.2367\n",
      "Epoch [10/20], Step [36/162], Loss: 0.1919\n",
      "Epoch [10/20], Step [41/162], Loss: 0.5802\n",
      "Epoch [10/20], Step [46/162], Loss: 0.1357\n",
      "Epoch [10/20], Step [51/162], Loss: 0.1883\n",
      "Epoch [10/20], Step [56/162], Loss: 0.4217\n",
      "Epoch [10/20], Step [61/162], Loss: 0.4097\n",
      "Epoch [10/20], Step [66/162], Loss: 0.2430\n",
      "Epoch [10/20], Step [71/162], Loss: 0.4396\n",
      "Epoch [10/20], Step [76/162], Loss: 0.1581\n",
      "Epoch [10/20], Step [81/162], Loss: 0.2787\n",
      "Epoch [10/20], Step [86/162], Loss: 0.1972\n",
      "Epoch [10/20], Step [91/162], Loss: 0.1932\n",
      "Epoch [10/20], Step [96/162], Loss: 0.2238\n",
      "Epoch [10/20], Step [101/162], Loss: 0.0799\n",
      "Epoch [10/20], Step [106/162], Loss: 0.3930\n",
      "Epoch [10/20], Step [111/162], Loss: 0.2092\n",
      "Epoch [10/20], Step [116/162], Loss: 0.3971\n",
      "Epoch [10/20], Step [121/162], Loss: 0.1133\n",
      "Epoch [10/20], Step [126/162], Loss: 0.1492\n",
      "Epoch [10/20], Step [131/162], Loss: 0.2371\n",
      "Epoch [10/20], Step [136/162], Loss: 0.1943\n",
      "Epoch [10/20], Step [141/162], Loss: 0.4293\n",
      "Epoch [10/20], Step [146/162], Loss: 0.3106\n",
      "Epoch [10/20], Step [151/162], Loss: 0.2269\n",
      "Epoch [10/20], Step [156/162], Loss: 0.2780\n",
      "Epoch [10/20], Step [161/162], Loss: 0.1021\n",
      "ðŸ”¥ Epoch [10/20] -> Loss: 0.2388, Accuracy: 91.08%\n",
      "Epoch [11/20], Step [1/162], Loss: 0.2599\n",
      "Epoch [11/20], Step [6/162], Loss: 0.2096\n",
      "Epoch [11/20], Step [11/162], Loss: 0.2290\n",
      "Epoch [11/20], Step [16/162], Loss: 0.0692\n",
      "Epoch [11/20], Step [21/162], Loss: 0.1120\n",
      "Epoch [11/20], Step [26/162], Loss: 0.1437\n",
      "Epoch [11/20], Step [31/162], Loss: 0.2854\n",
      "Epoch [11/20], Step [36/162], Loss: 0.1354\n",
      "Epoch [11/20], Step [41/162], Loss: 0.1664\n",
      "Epoch [11/20], Step [46/162], Loss: 0.1247\n",
      "Epoch [11/20], Step [51/162], Loss: 0.1338\n",
      "Epoch [11/20], Step [56/162], Loss: 0.1851\n",
      "Epoch [11/20], Step [61/162], Loss: 0.2689\n",
      "Epoch [11/20], Step [66/162], Loss: 0.0686\n",
      "Epoch [11/20], Step [71/162], Loss: 0.1487\n",
      "Epoch [11/20], Step [76/162], Loss: 0.0448\n",
      "Epoch [11/20], Step [81/162], Loss: 0.1423\n",
      "Epoch [11/20], Step [86/162], Loss: 0.2253\n",
      "Epoch [11/20], Step [91/162], Loss: 0.0821\n",
      "Epoch [11/20], Step [96/162], Loss: 0.3773\n",
      "Epoch [11/20], Step [101/162], Loss: 0.1963\n",
      "Epoch [11/20], Step [106/162], Loss: 0.2065\n",
      "Epoch [11/20], Step [111/162], Loss: 0.1041\n",
      "Epoch [11/20], Step [116/162], Loss: 0.1847\n",
      "Epoch [11/20], Step [121/162], Loss: 0.1394\n",
      "Epoch [11/20], Step [126/162], Loss: 0.0820\n",
      "Epoch [11/20], Step [131/162], Loss: 0.3041\n",
      "Epoch [11/20], Step [136/162], Loss: 0.1744\n",
      "Epoch [11/20], Step [141/162], Loss: 0.2724\n",
      "Epoch [11/20], Step [146/162], Loss: 0.0995\n",
      "Epoch [11/20], Step [151/162], Loss: 0.1697\n",
      "Epoch [11/20], Step [156/162], Loss: 0.0861\n",
      "Epoch [11/20], Step [161/162], Loss: 0.0945\n",
      "ðŸ”¥ Epoch [11/20] -> Loss: 0.2266, Accuracy: 91.29%\n",
      "Epoch [12/20], Step [1/162], Loss: 0.1125\n",
      "Epoch [12/20], Step [6/162], Loss: 0.1015\n",
      "Epoch [12/20], Step [11/162], Loss: 0.1361\n",
      "Epoch [12/20], Step [16/162], Loss: 0.2636\n",
      "Epoch [12/20], Step [21/162], Loss: 0.4026\n",
      "Epoch [12/20], Step [26/162], Loss: 0.7544\n",
      "Epoch [12/20], Step [31/162], Loss: 0.1659\n",
      "Epoch [12/20], Step [36/162], Loss: 0.1106\n",
      "Epoch [12/20], Step [41/162], Loss: 0.1366\n",
      "Epoch [12/20], Step [46/162], Loss: 0.0896\n",
      "Epoch [12/20], Step [51/162], Loss: 0.2865\n",
      "Epoch [12/20], Step [56/162], Loss: 0.4999\n",
      "Epoch [12/20], Step [61/162], Loss: 0.2913\n",
      "Epoch [12/20], Step [66/162], Loss: 0.1280\n",
      "Epoch [12/20], Step [71/162], Loss: 0.2650\n",
      "Epoch [12/20], Step [76/162], Loss: 0.2942\n",
      "Epoch [12/20], Step [81/162], Loss: 0.0736\n",
      "Epoch [12/20], Step [86/162], Loss: 0.0892\n",
      "Epoch [12/20], Step [91/162], Loss: 0.1608\n",
      "Epoch [12/20], Step [96/162], Loss: 0.1545\n",
      "Epoch [12/20], Step [101/162], Loss: 0.1592\n",
      "Epoch [12/20], Step [106/162], Loss: 0.1422\n",
      "Epoch [12/20], Step [111/162], Loss: 0.2306\n",
      "Epoch [12/20], Step [116/162], Loss: 0.0973\n",
      "Epoch [12/20], Step [121/162], Loss: 0.2481\n",
      "Epoch [12/20], Step [126/162], Loss: 0.0998\n",
      "Epoch [12/20], Step [131/162], Loss: 0.2701\n",
      "Epoch [12/20], Step [136/162], Loss: 0.1439\n",
      "Epoch [12/20], Step [141/162], Loss: 0.1753\n",
      "Epoch [12/20], Step [146/162], Loss: 0.3174\n",
      "Epoch [12/20], Step [151/162], Loss: 0.0753\n",
      "Epoch [12/20], Step [156/162], Loss: 0.1294\n",
      "Epoch [12/20], Step [161/162], Loss: 0.1020\n",
      "ðŸ”¥ Epoch [12/20] -> Loss: 0.2282, Accuracy: 91.22%\n",
      "Epoch [13/20], Step [1/162], Loss: 0.1101\n",
      "Epoch [13/20], Step [6/162], Loss: 0.2837\n",
      "Epoch [13/20], Step [11/162], Loss: 0.2180\n",
      "Epoch [13/20], Step [16/162], Loss: 0.3318\n",
      "Epoch [13/20], Step [21/162], Loss: 0.2338\n",
      "Epoch [13/20], Step [26/162], Loss: 0.7990\n",
      "Epoch [13/20], Step [31/162], Loss: 0.4187\n",
      "Epoch [13/20], Step [36/162], Loss: 0.1824\n",
      "Epoch [13/20], Step [41/162], Loss: 0.3396\n",
      "Epoch [13/20], Step [46/162], Loss: 0.2582\n",
      "Epoch [13/20], Step [51/162], Loss: 0.2192\n",
      "Epoch [13/20], Step [56/162], Loss: 0.1979\n",
      "Epoch [13/20], Step [61/162], Loss: 0.1361\n",
      "Epoch [13/20], Step [66/162], Loss: 0.4686\n",
      "Epoch [13/20], Step [71/162], Loss: 0.2971\n",
      "Epoch [13/20], Step [76/162], Loss: 0.1405\n",
      "Epoch [13/20], Step [81/162], Loss: 0.3286\n",
      "Epoch [13/20], Step [86/162], Loss: 0.2120\n",
      "Epoch [13/20], Step [91/162], Loss: 0.1384\n",
      "Epoch [13/20], Step [96/162], Loss: 0.0713\n",
      "Epoch [13/20], Step [101/162], Loss: 0.1279\n",
      "Epoch [13/20], Step [106/162], Loss: 0.0878\n",
      "Epoch [13/20], Step [111/162], Loss: 0.1105\n",
      "Epoch [13/20], Step [116/162], Loss: 0.7119\n",
      "Epoch [13/20], Step [121/162], Loss: 0.3504\n",
      "Epoch [13/20], Step [126/162], Loss: 0.1398\n",
      "Epoch [13/20], Step [131/162], Loss: 0.0823\n",
      "Epoch [13/20], Step [136/162], Loss: 0.1498\n",
      "Epoch [13/20], Step [141/162], Loss: 0.2811\n",
      "Epoch [13/20], Step [146/162], Loss: 0.3091\n",
      "Epoch [13/20], Step [151/162], Loss: 0.2325\n",
      "Epoch [13/20], Step [156/162], Loss: 0.2260\n",
      "Epoch [13/20], Step [161/162], Loss: 0.1925\n",
      "ðŸ”¥ Epoch [13/20] -> Loss: 0.2066, Accuracy: 92.05%\n",
      "Epoch [14/20], Step [1/162], Loss: 0.2917\n",
      "Epoch [14/20], Step [6/162], Loss: 0.0640\n",
      "Epoch [14/20], Step [11/162], Loss: 0.2677\n",
      "Epoch [14/20], Step [16/162], Loss: 0.0891\n",
      "Epoch [14/20], Step [21/162], Loss: 0.2481\n",
      "Epoch [14/20], Step [26/162], Loss: 0.2040\n",
      "Epoch [14/20], Step [31/162], Loss: 0.1899\n",
      "Epoch [14/20], Step [36/162], Loss: 0.1180\n",
      "Epoch [14/20], Step [41/162], Loss: 0.2156\n",
      "Epoch [14/20], Step [46/162], Loss: 0.0542\n",
      "Epoch [14/20], Step [51/162], Loss: 0.1183\n",
      "Epoch [14/20], Step [56/162], Loss: 0.3556\n",
      "Epoch [14/20], Step [61/162], Loss: 0.5293\n",
      "Epoch [14/20], Step [66/162], Loss: 0.0738\n",
      "Epoch [14/20], Step [71/162], Loss: 0.2183\n",
      "Epoch [14/20], Step [76/162], Loss: 0.1666\n",
      "Epoch [14/20], Step [81/162], Loss: 0.0999\n",
      "Epoch [14/20], Step [86/162], Loss: 0.1263\n",
      "Epoch [14/20], Step [91/162], Loss: 0.1089\n",
      "Epoch [14/20], Step [96/162], Loss: 0.3132\n",
      "Epoch [14/20], Step [101/162], Loss: 0.2797\n",
      "Epoch [14/20], Step [106/162], Loss: 0.4742\n",
      "Epoch [14/20], Step [111/162], Loss: 0.1473\n",
      "Epoch [14/20], Step [116/162], Loss: 0.1285\n",
      "Epoch [14/20], Step [121/162], Loss: 0.2278\n",
      "Epoch [14/20], Step [126/162], Loss: 0.4657\n",
      "Epoch [14/20], Step [131/162], Loss: 0.3558\n",
      "Epoch [14/20], Step [136/162], Loss: 0.2650\n",
      "Epoch [14/20], Step [141/162], Loss: 0.1613\n",
      "Epoch [14/20], Step [146/162], Loss: 0.2081\n",
      "Epoch [14/20], Step [151/162], Loss: 0.1900\n",
      "Epoch [14/20], Step [156/162], Loss: 0.0956\n",
      "Epoch [14/20], Step [161/162], Loss: 0.1443\n",
      "ðŸ”¥ Epoch [14/20] -> Loss: 0.2036, Accuracy: 91.87%\n",
      "Epoch [15/20], Step [1/162], Loss: 0.1208\n",
      "Epoch [15/20], Step [6/162], Loss: 0.2372\n",
      "Epoch [15/20], Step [11/162], Loss: 0.1677\n",
      "Epoch [15/20], Step [16/162], Loss: 0.1191\n",
      "Epoch [15/20], Step [21/162], Loss: 0.0823\n",
      "Epoch [15/20], Step [26/162], Loss: 0.3377\n",
      "Epoch [15/20], Step [31/162], Loss: 0.1923\n",
      "Epoch [15/20], Step [36/162], Loss: 0.0606\n",
      "Epoch [15/20], Step [41/162], Loss: 0.0808\n",
      "Epoch [15/20], Step [46/162], Loss: 0.1008\n",
      "Epoch [15/20], Step [51/162], Loss: 0.0544\n",
      "Epoch [15/20], Step [56/162], Loss: 0.2023\n",
      "Epoch [15/20], Step [61/162], Loss: 0.0881\n",
      "Epoch [15/20], Step [66/162], Loss: 0.0908\n",
      "Epoch [15/20], Step [71/162], Loss: 0.1883\n",
      "Epoch [15/20], Step [76/162], Loss: 0.0302\n",
      "Epoch [15/20], Step [81/162], Loss: 0.2846\n",
      "Epoch [15/20], Step [86/162], Loss: 0.0927\n",
      "Epoch [15/20], Step [91/162], Loss: 0.2113\n",
      "Epoch [15/20], Step [96/162], Loss: 0.0772\n",
      "Epoch [15/20], Step [101/162], Loss: 0.1524\n",
      "Epoch [15/20], Step [106/162], Loss: 0.4581\n",
      "Epoch [15/20], Step [111/162], Loss: 0.1504\n",
      "Epoch [15/20], Step [116/162], Loss: 0.3997\n",
      "Epoch [15/20], Step [121/162], Loss: 0.1079\n",
      "Epoch [15/20], Step [126/162], Loss: 0.0985\n",
      "Epoch [15/20], Step [131/162], Loss: 0.3727\n",
      "Epoch [15/20], Step [136/162], Loss: 0.3626\n",
      "Epoch [15/20], Step [141/162], Loss: 0.1767\n",
      "Epoch [15/20], Step [146/162], Loss: 0.4176\n",
      "Epoch [15/20], Step [151/162], Loss: 0.1377\n",
      "Epoch [15/20], Step [156/162], Loss: 0.4902\n",
      "Epoch [15/20], Step [161/162], Loss: 0.2019\n",
      "ðŸ”¥ Epoch [15/20] -> Loss: 0.1982, Accuracy: 92.55%\n",
      "Epoch [16/20], Step [1/162], Loss: 0.1215\n",
      "Epoch [16/20], Step [6/162], Loss: 0.2918\n",
      "Epoch [16/20], Step [11/162], Loss: 0.1929\n",
      "Epoch [16/20], Step [16/162], Loss: 0.1366\n",
      "Epoch [16/20], Step [21/162], Loss: 0.3235\n",
      "Epoch [16/20], Step [26/162], Loss: 0.1780\n",
      "Epoch [16/20], Step [31/162], Loss: 0.1684\n",
      "Epoch [16/20], Step [36/162], Loss: 0.2353\n",
      "Epoch [16/20], Step [41/162], Loss: 0.1701\n",
      "Epoch [16/20], Step [46/162], Loss: 0.0636\n",
      "Epoch [16/20], Step [51/162], Loss: 0.2656\n",
      "Epoch [16/20], Step [56/162], Loss: 0.1245\n",
      "Epoch [16/20], Step [61/162], Loss: 0.1217\n",
      "Epoch [16/20], Step [66/162], Loss: 0.0544\n",
      "Epoch [16/20], Step [71/162], Loss: 0.1999\n",
      "Epoch [16/20], Step [76/162], Loss: 0.1060\n",
      "Epoch [16/20], Step [81/162], Loss: 0.0761\n",
      "Epoch [16/20], Step [86/162], Loss: 0.1449\n",
      "Epoch [16/20], Step [91/162], Loss: 0.0395\n",
      "Epoch [16/20], Step [96/162], Loss: 0.1994\n",
      "Epoch [16/20], Step [101/162], Loss: 0.1353\n",
      "Epoch [16/20], Step [106/162], Loss: 0.3139\n",
      "Epoch [16/20], Step [111/162], Loss: 0.2295\n",
      "Epoch [16/20], Step [116/162], Loss: 0.2526\n",
      "Epoch [16/20], Step [121/162], Loss: 0.1926\n",
      "Epoch [16/20], Step [126/162], Loss: 0.1365\n",
      "Epoch [16/20], Step [131/162], Loss: 0.1714\n",
      "Epoch [16/20], Step [136/162], Loss: 0.1520\n",
      "Epoch [16/20], Step [141/162], Loss: 0.3085\n",
      "Epoch [16/20], Step [146/162], Loss: 0.1813\n",
      "Epoch [16/20], Step [151/162], Loss: 0.1612\n",
      "Epoch [16/20], Step [156/162], Loss: 0.0656\n",
      "Epoch [16/20], Step [161/162], Loss: 0.1898\n",
      "ðŸ”¥ Epoch [16/20] -> Loss: 0.1922, Accuracy: 92.59%\n",
      "Epoch [17/20], Step [1/162], Loss: 0.2361\n",
      "Epoch [17/20], Step [6/162], Loss: 0.1466\n",
      "Epoch [17/20], Step [11/162], Loss: 0.2043\n",
      "Epoch [17/20], Step [16/162], Loss: 0.1975\n",
      "Epoch [17/20], Step [21/162], Loss: 0.1384\n",
      "Epoch [17/20], Step [26/162], Loss: 0.0862\n",
      "Epoch [17/20], Step [31/162], Loss: 0.0801\n",
      "Epoch [17/20], Step [36/162], Loss: 0.1687\n",
      "Epoch [17/20], Step [41/162], Loss: 0.1153\n",
      "Epoch [17/20], Step [46/162], Loss: 0.4424\n",
      "Epoch [17/20], Step [51/162], Loss: 0.0922\n",
      "Epoch [17/20], Step [56/162], Loss: 0.2717\n",
      "Epoch [17/20], Step [61/162], Loss: 0.1239\n",
      "Epoch [17/20], Step [66/162], Loss: 0.2526\n",
      "Epoch [17/20], Step [71/162], Loss: 0.2429\n",
      "Epoch [17/20], Step [76/162], Loss: 0.1082\n",
      "Epoch [17/20], Step [81/162], Loss: 0.0820\n",
      "Epoch [17/20], Step [86/162], Loss: 0.1863\n",
      "Epoch [17/20], Step [91/162], Loss: 0.1451\n",
      "Epoch [17/20], Step [96/162], Loss: 0.1569\n",
      "Epoch [17/20], Step [101/162], Loss: 0.1444\n",
      "Epoch [17/20], Step [106/162], Loss: 0.1084\n",
      "Epoch [17/20], Step [111/162], Loss: 0.2606\n",
      "Epoch [17/20], Step [116/162], Loss: 0.0778\n",
      "Epoch [17/20], Step [121/162], Loss: 0.1786\n",
      "Epoch [17/20], Step [126/162], Loss: 0.2881\n",
      "Epoch [17/20], Step [131/162], Loss: 0.2564\n",
      "Epoch [17/20], Step [136/162], Loss: 0.1709\n",
      "Epoch [17/20], Step [141/162], Loss: 0.2500\n",
      "Epoch [17/20], Step [146/162], Loss: 0.2442\n",
      "Epoch [17/20], Step [151/162], Loss: 0.2263\n",
      "Epoch [17/20], Step [156/162], Loss: 0.1068\n",
      "Epoch [17/20], Step [161/162], Loss: 0.1857\n",
      "ðŸ”¥ Epoch [17/20] -> Loss: 0.1981, Accuracy: 92.22%\n",
      "Epoch [18/20], Step [1/162], Loss: 0.2129\n",
      "Epoch [18/20], Step [6/162], Loss: 0.0338\n",
      "Epoch [18/20], Step [11/162], Loss: 0.2264\n",
      "Epoch [18/20], Step [16/162], Loss: 0.1211\n",
      "Epoch [18/20], Step [21/162], Loss: 0.1358\n",
      "Epoch [18/20], Step [26/162], Loss: 0.2849\n",
      "Epoch [18/20], Step [31/162], Loss: 0.3065\n",
      "Epoch [18/20], Step [36/162], Loss: 0.5827\n",
      "Epoch [18/20], Step [41/162], Loss: 0.3202\n",
      "Epoch [18/20], Step [46/162], Loss: 0.2717\n",
      "Epoch [18/20], Step [51/162], Loss: 0.2317\n",
      "Epoch [18/20], Step [56/162], Loss: 0.1801\n",
      "Epoch [18/20], Step [61/162], Loss: 0.1133\n",
      "Epoch [18/20], Step [66/162], Loss: 0.0906\n",
      "Epoch [18/20], Step [71/162], Loss: 0.4316\n",
      "Epoch [18/20], Step [76/162], Loss: 0.2662\n",
      "Epoch [18/20], Step [81/162], Loss: 0.2162\n",
      "Epoch [18/20], Step [86/162], Loss: 0.0570\n",
      "Epoch [18/20], Step [91/162], Loss: 0.2026\n",
      "Epoch [18/20], Step [96/162], Loss: 0.2784\n",
      "Epoch [18/20], Step [101/162], Loss: 0.2802\n",
      "Epoch [18/20], Step [106/162], Loss: 0.2100\n",
      "Epoch [18/20], Step [111/162], Loss: 0.1396\n",
      "Epoch [18/20], Step [116/162], Loss: 0.0933\n",
      "Epoch [18/20], Step [121/162], Loss: 0.2220\n",
      "Epoch [18/20], Step [126/162], Loss: 0.2540\n",
      "Epoch [18/20], Step [131/162], Loss: 0.1459\n",
      "Epoch [18/20], Step [136/162], Loss: 0.2398\n",
      "Epoch [18/20], Step [141/162], Loss: 0.1611\n",
      "Epoch [18/20], Step [146/162], Loss: 0.2472\n",
      "Epoch [18/20], Step [151/162], Loss: 0.1368\n",
      "Epoch [18/20], Step [156/162], Loss: 0.1017\n",
      "Epoch [18/20], Step [161/162], Loss: 0.1224\n",
      "ðŸ”¥ Epoch [18/20] -> Loss: 0.1871, Accuracy: 92.65%\n",
      "Epoch [19/20], Step [1/162], Loss: 0.1733\n",
      "Epoch [19/20], Step [6/162], Loss: 0.0426\n",
      "Epoch [19/20], Step [11/162], Loss: 0.1289\n",
      "Epoch [19/20], Step [16/162], Loss: 0.1373\n",
      "Epoch [19/20], Step [21/162], Loss: 0.1861\n",
      "Epoch [19/20], Step [26/162], Loss: 0.1559\n",
      "Epoch [19/20], Step [31/162], Loss: 0.1209\n",
      "Epoch [19/20], Step [36/162], Loss: 0.0485\n",
      "Epoch [19/20], Step [41/162], Loss: 0.2968\n",
      "Epoch [19/20], Step [46/162], Loss: 0.1390\n",
      "Epoch [19/20], Step [51/162], Loss: 0.0407\n",
      "Epoch [19/20], Step [56/162], Loss: 0.1268\n",
      "Epoch [19/20], Step [61/162], Loss: 0.1894\n",
      "Epoch [19/20], Step [66/162], Loss: 0.4453\n",
      "Epoch [19/20], Step [71/162], Loss: 0.1136\n",
      "Epoch [19/20], Step [76/162], Loss: 0.1655\n",
      "Epoch [19/20], Step [81/162], Loss: 0.1084\n",
      "Epoch [19/20], Step [86/162], Loss: 0.0874\n",
      "Epoch [19/20], Step [91/162], Loss: 0.1494\n",
      "Epoch [19/20], Step [96/162], Loss: 0.0907\n",
      "Epoch [19/20], Step [101/162], Loss: 0.0547\n",
      "Epoch [19/20], Step [106/162], Loss: 0.3468\n",
      "Epoch [19/20], Step [111/162], Loss: 0.4272\n",
      "Epoch [19/20], Step [116/162], Loss: 0.1809\n",
      "Epoch [19/20], Step [121/162], Loss: 0.2394\n",
      "Epoch [19/20], Step [126/162], Loss: 0.1519\n",
      "Epoch [19/20], Step [131/162], Loss: 0.1940\n",
      "Epoch [19/20], Step [136/162], Loss: 0.2286\n",
      "Epoch [19/20], Step [141/162], Loss: 0.1845\n",
      "Epoch [19/20], Step [146/162], Loss: 0.2866\n",
      "Epoch [19/20], Step [151/162], Loss: 0.1241\n",
      "Epoch [19/20], Step [156/162], Loss: 0.4904\n",
      "Epoch [19/20], Step [161/162], Loss: 0.2914\n",
      "ðŸ”¥ Epoch [19/20] -> Loss: 0.2021, Accuracy: 92.38%\n",
      "Epoch [20/20], Step [1/162], Loss: 0.0720\n",
      "Epoch [20/20], Step [6/162], Loss: 0.1414\n",
      "Epoch [20/20], Step [11/162], Loss: 0.0885\n",
      "Epoch [20/20], Step [16/162], Loss: 0.3405\n",
      "Epoch [20/20], Step [21/162], Loss: 0.2624\n",
      "Epoch [20/20], Step [26/162], Loss: 0.3034\n",
      "Epoch [20/20], Step [31/162], Loss: 0.1717\n",
      "Epoch [20/20], Step [36/162], Loss: 0.1710\n",
      "Epoch [20/20], Step [41/162], Loss: 0.5141\n",
      "Epoch [20/20], Step [46/162], Loss: 0.2741\n",
      "Epoch [20/20], Step [51/162], Loss: 0.0947\n",
      "Epoch [20/20], Step [56/162], Loss: 0.2274\n",
      "Epoch [20/20], Step [61/162], Loss: 0.1221\n",
      "Epoch [20/20], Step [66/162], Loss: 0.2595\n",
      "Epoch [20/20], Step [71/162], Loss: 0.0400\n",
      "Epoch [20/20], Step [76/162], Loss: 0.1137\n",
      "Epoch [20/20], Step [81/162], Loss: 0.3901\n",
      "Epoch [20/20], Step [86/162], Loss: 0.0298\n",
      "Epoch [20/20], Step [91/162], Loss: 0.1294\n",
      "Epoch [20/20], Step [96/162], Loss: 0.1546\n",
      "Epoch [20/20], Step [101/162], Loss: 0.3134\n",
      "Epoch [20/20], Step [106/162], Loss: 0.1662\n",
      "Epoch [20/20], Step [111/162], Loss: 0.1893\n",
      "Epoch [20/20], Step [116/162], Loss: 0.1225\n",
      "Epoch [20/20], Step [121/162], Loss: 0.2667\n",
      "Epoch [20/20], Step [126/162], Loss: 0.2736\n",
      "Epoch [20/20], Step [131/162], Loss: 0.3584\n",
      "Epoch [20/20], Step [136/162], Loss: 0.1053\n",
      "Epoch [20/20], Step [141/162], Loss: 0.3648\n",
      "Epoch [20/20], Step [146/162], Loss: 0.0910\n",
      "Epoch [20/20], Step [151/162], Loss: 0.1164\n",
      "Epoch [20/20], Step [156/162], Loss: 0.2126\n",
      "Epoch [20/20], Step [161/162], Loss: 0.0893\n",
      "ðŸ”¥ Epoch [20/20] -> Loss: 0.1775, Accuracy: 93.81%\n",
      "âœ… Model saved successfully!\n",
      "ðŸŽ¯ Test Accuracy: 47.33%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "47.33178654292343"
      ]
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# # Example Data (Ensure your real data is loaded correctly)\n",
    "# spectrogram_data = np.random.rand(1000, 28, 5, 1280)  # Simulated input\n",
    "# y = np.random.randint(0, 2, (1000,))  # Simulated binary classification labels\n",
    "\n",
    "# # Train-Test Split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(spectrogram_data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Convert to PyTorch Tensors\n",
    "# X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "# X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "# y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "# y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# # Create DataLoader objects\n",
    "batch_size = 32\n",
    "# train_dataset = TensorDataset(X_train, y_train)\n",
    "# test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, Loss Function, and Optimizer\n",
    "model = TGCNN().to(device)  # Move model to GPU if available\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Parameters\n",
    "num_epochs = 20  # Adjust based on dataset size\n",
    "print_interval = 5  # Print every 5 batches\n",
    "\n",
    "# ðŸš€ Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        \n",
    "        loss = criterion(outputs, targets)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # Optional: Gradient clipping\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        if batch_idx % print_interval == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"ðŸ”¥ Epoch [{epoch+1}/{num_epochs}] -> Loss: {total_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"TGCNN_model.pth\")\n",
    "print(\"âœ… Model saved successfully!\")\n",
    "\n",
    "# ðŸš€ Evaluation Function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"ðŸŽ¯ Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Run Evaluation\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
