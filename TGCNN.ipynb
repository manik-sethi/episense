{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Mock Data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Creation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNUnit(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=16):\n",
    "        super(CNNUnit, self).__init__()\n",
    "        \n",
    "        # First 3x3 Conv Layer\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        \n",
    "        # Second 3x3 Conv Layer \n",
    "        self.conv2 = nn.Conv2d(out_channels, out_channels, kernel_size=3, padding=1)\n",
    "        self.bn2 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu2 = nn.GELU()\n",
    "\n",
    "        # Third 3x3 Conv Layer with Stride 2 (Downsampling)\n",
    "        self.conv3 = nn.Conv2d(out_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn3 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu3 = nn.GELU()\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        # First Conv2D + GELU + BatchNorm\n",
    "        x = self.gelu1(self.bn1(self.conv1(x)))  \n",
    "        \n",
    "        # Second Conv2D + GELU + BatchNorm\n",
    "        x = self.gelu2(self.bn2(self.conv2(x)))  \n",
    "        \n",
    "        # Third Conv2D (Stride=2 for downsampling) + GELU + BatchNorm\n",
    "        x = self.gelu3(self.bn3(self.conv3(x)))  \n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class CNNLayer(nn.Module):\n",
    "    def __init__(self, in_channels, out_channels=24):\n",
    "        super(CNNLayer, self).__init__()\n",
    "\n",
    "        self.conv1 = nn.Conv2d(in_channels, out_channels, kernel_size=3, stride=2, padding=1)\n",
    "        self.bn1 = nn.BatchNorm2d(out_channels)\n",
    "        self.gelu1 = nn.GELU()\n",
    "        \n",
    "    def forward(self, x):\n",
    "\n",
    "        x = self.gelu1(self.bn1(self.conv1(x)))\n",
    "        return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LocalInformationLayer(nn.Module):\n",
    "    def __init__(self, in_channels=24):\n",
    "        super(LocalInformationLayer, self).__init__()\n",
    "        self.conv = nn.Conv2d(in_channels, in_channels, kernel_size=3, padding=1, bias=False)\n",
    "\n",
    "    def forward(self, x):\n",
    "        return self.conv(x) + x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "class SMHSA(nn.Module):\n",
    "    def __init__(self, dim, heads=8, kernel_size=3, stride=2, bias=True):\n",
    "        super(SMHSA, self).__init__()\n",
    "\n",
    "        assert dim % heads == 0, \"dim must be divisible by heads\"\n",
    "\n",
    "        self.dim = dim\n",
    "        self.heads = heads\n",
    "        self.scale = (dim // heads) ** -0.5  # Scaling factor for attention\n",
    "\n",
    "        # Linear layers to compute Query, Key, and Value\n",
    "        self.to_qkv = nn.Linear(dim, dim * 3, bias=False)\n",
    "\n",
    "        # Depth-wise convolution for Q and K downsampling\n",
    "        self.spatial_reduction = nn.Conv2d(\n",
    "            dim, dim, kernel_size=kernel_size, stride=stride, padding=kernel_size//2, \n",
    "            groups=dim, bias=False\n",
    "        )\n",
    "\n",
    "        # Initialize bias with proper shape for attention\n",
    "        self.bias = nn.Parameter(torch.zeros(1, heads, 1, 1)) if bias else None\n",
    "\n",
    "        # Final projection after self-attention\n",
    "        self.to_out = nn.Linear(dim, dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "        B, C, H, W = x.shape\n",
    "        \n",
    "        # Reshape input to sequence format\n",
    "        x = x.permute(0, 2, 3, 1).reshape(B, H * W, C)  # (B, seq_len, C)\n",
    "\n",
    "        # Compute Query, Key, and Value\n",
    "        qkv = self.to_qkv(x)\n",
    "        q, k, v = qkv.chunk(3, dim=-1)\n",
    "\n",
    "        # Reshape for multi-head attention\n",
    "        q = q.view(B, -1, self.heads, C // self.heads).permute(0, 2, 1, 3)\n",
    "        k = k.view(B, -1, self.heads, C // self.heads).permute(0, 2, 1, 3)\n",
    "        v = v.view(B, -1, self.heads, C // self.heads).permute(0, 2, 1, 3)\n",
    "\n",
    "        # ðŸ”¹ **Fix: Ensure correct reshaping before Conv2D**\n",
    "        q = q.permute(0, 1, 3, 2).reshape(B, C, H, W)\n",
    "        k = k.permute(0, 1, 3, 2).reshape(B, C, H, W)\n",
    "\n",
    "        # Apply spatial reduction\n",
    "        q = self.spatial_reduction(q)\n",
    "        k = self.spatial_reduction(k)\n",
    "\n",
    "        # Get reduced spatial dimensions after Conv2D\n",
    "        H_reduced, W_reduced = q.shape[2], q.shape[3]\n",
    "\n",
    "        # Reshape back for attention\n",
    "        q = q.reshape(B, self.heads, C // self.heads, H_reduced * W_reduced).permute(0, 1, 3, 2)\n",
    "        k = k.reshape(B, self.heads, C // self.heads, H_reduced * W_reduced).permute(0, 1, 3, 2)\n",
    "\n",
    "        # Compute attention scores\n",
    "        attn = torch.matmul(q, k.transpose(-2, -1)) * self.scale\n",
    "\n",
    "        # Apply bias if enabled\n",
    "        if self.bias is not None:\n",
    "            # Expand bias to match attention dimensions\n",
    "            bias_expanded = self.bias.expand(B, self.heads, H_reduced * W_reduced, H_reduced * W_reduced)\n",
    "            attn = attn + bias_expanded\n",
    "\n",
    "        attn = F.softmax(attn, dim=-1)\n",
    "\n",
    "        # Apply attention to Value\n",
    "        out = torch.matmul(attn, v)\n",
    "\n",
    "        # Reshape output back to spatial format\n",
    "        out = out.permute(0, 2, 1, 3).reshape(B, H_reduced * W_reduced, C)\n",
    "        out = self.to_out(out)\n",
    "        out = out.reshape(B, H_reduced, W_reduced, C).permute(0, 3, 1, 2)\n",
    "\n",
    "        return out\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RFFN(nn.Module):\n",
    "    def __init__(self, dim, expansion=4):\n",
    "        super(RFFN, self).__init__()\n",
    "        \n",
    "        hidden_dim = dim * expansion\n",
    "\n",
    "        self.conv1 = nn.Conv2d(dim, hidden_dim, kernel_size=1, padding=0, bias=False)\n",
    "        self.bn1 = nn.BatchNorm2d(hidden_dim)\n",
    "        self.gelu1 = nn.GELU()\n",
    "\n",
    "        self.conv2 = nn.Conv2d(hidden_dim, hidden_dim, kernel_size=3, padding=1, bias=False)\n",
    "        self.bn2 = nn.BatchNorm2d(hidden_dim)\n",
    "        self.gelu2 = nn.GELU()\n",
    "\n",
    "        self.conv3 = nn.Conv2d(hidden_dim, dim, kernel_size=1, padding=0, bias=False)\n",
    "        self.bn3 = nn.BatchNorm2d(dim)\n",
    "\n",
    "    def forward(self, x):\n",
    "\n",
    "        residual = x\n",
    "        x = self.gelu1(self.bn1(self.conv1(x)))\n",
    "        x = self.gelu2(self.bn2(self.conv2(x)))\n",
    "        x = self.bn3(self.conv3(x))\n",
    "\n",
    "        return x + residual\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "\n",
    "class FC(nn.Module):\n",
    "    def __init__(self, in_features):\n",
    "        \"\"\"\n",
    "        Fully Connected (FC) Layer for Classification\n",
    "\n",
    "        Args:\n",
    "        - in_features: Number of input features (from GAP output, i.e., number of channels)\n",
    "        \"\"\"\n",
    "        super(FC, self).__init__()\n",
    "\n",
    "        self.pool = nn.AdaptiveAvgPool2d((1, 1))  # Global Average Pooling\n",
    "\n",
    "        self.fc1 = nn.Linear(in_features, 512)  # First Fully Connected Layer\n",
    "        self.relu1 = nn.ReLU()\n",
    "        self.dropout = nn.Dropout(0.5)  # Dropout to prevent overfitting\n",
    "        self.fc2 = nn.Linear(512, 2)  # Output Layer (Binary Classification)\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.pool(x)  # Apply Global Average Pooling\n",
    "        x = torch.flatten(x, start_dim=1)  # Flatten from (B, C, 1, 1) -> (B, C)\n",
    "        x = self.relu1(self.fc1(x))  # First Fully Connected Layer + ReLU\n",
    "        x = self.dropout(x)  # Apply Dropout\n",
    "        x = self.fc2(x)  # Final Output Layer\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "class TGCNN(nn.Module):\n",
    "    def __init__(self):\n",
    "        super(TGCNN, self).__init__()\n",
    "\n",
    "        # CNN Layers\n",
    "        self.cnn_unit = CNNUnit(in_channels=22, out_channels=16)\n",
    "        self.cnn_layer1 = CNNLayer(in_channels=16, out_channels=24)\n",
    "        self.cnn_layer2 = CNNLayer(in_channels=24, out_channels=32)\n",
    "\n",
    "        # Transformer Block 1\n",
    "        self.LIL1 = LocalInformationLayer(in_channels=24)\n",
    "        self.norm1a = nn.LayerNorm([24, 2, 320])\n",
    "\n",
    "        self.multiheaded_att1 = SMHSA(dim=24, stride=1)  # Adjust stride if needed\n",
    "        self.norm1b = nn.LayerNorm([24, 2, 320])\n",
    "        self.recurrent1 = RFFN(dim=24)\n",
    "\n",
    "        # Transformer Block 2\n",
    "        self.LIL2 = LocalInformationLayer(in_channels=32)\n",
    "        self.norm1b = nn.LayerNorm([32, 1, 160])\n",
    "        self.multiheaded_att2 = SMHSA(dim=32, stride=1)  # Adjust stride if needed\n",
    "        self.norm1b = nn.LayerNorm([32, 1, 160])\n",
    "        self.recurrent2 = RFFN(dim=32)\n",
    "\n",
    "        # Global Average Pooling + Fully Connected Layer\n",
    "        self.fc = FC(in_features=32)  # Matches last CNN layer's output channels\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = x.unsqueeze(2)\n",
    "        x = self.cnn_unit(x)      \n",
    "        x = self.cnn_layer1(x)    \n",
    "        x = self.LIL1(x)          \n",
    "        x = self.multiheaded_att1(x)\n",
    "        x = self.recurrent1(x)    \n",
    "        x = self.cnn_layer2(x)    \n",
    "        x = self.LIL2(x)          \n",
    "        x = self.multiheaded_att2(x)\n",
    "        x = self.recurrent2(x)    \n",
    "        x = self.fc(x)\n",
    "\n",
    "        return x\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([100, 2])\n"
     ]
    }
   ],
   "source": [
    "data = torch.rand(100,22,1280)\n",
    "model = TGCNN()\n",
    "output = model(data)\n",
    "print(output.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Load in Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded: (1601, 22, 1471) (1601,)\n"
     ]
    }
   ],
   "source": [
    "import pickle\n",
    "pkl_file = os.path.join(\"processed_data\", \"final_data.pkl\")\n",
    "with open(pkl_file, \"rb\") as f:\n",
    "    data = pickle.load(f)\n",
    "\n",
    "X = data[\"X\"]          # shape (N, 22, 1471)\n",
    "y = data[\"y\"]          # shape (N,)\n",
    "\n",
    "print(\"Loaded:\", X.shape, y.shape)\n",
    "\n",
    "X = torch.tensor(X, dtype=torch.float32)   # <-- cast to float32\n",
    "y = torch.tensor(y, dtype=torch.long)  \n",
    "\n",
    "dataset = TensorDataset(X, y)\n",
    "\n",
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cuda\n",
      "Epoch [1/20], Step [1/40], Loss: 0.8719\n",
      "Epoch [1/20], Step [6/40], Loss: 0.2999\n",
      "Epoch [1/20], Step [11/40], Loss: 0.2387\n",
      "Epoch [1/20], Step [16/40], Loss: 0.1502\n",
      "Epoch [1/20], Step [21/40], Loss: 0.1054\n",
      "Epoch [1/20], Step [26/40], Loss: 0.2723\n",
      "Epoch [1/20], Step [31/40], Loss: 0.4487\n",
      "Epoch [1/20], Step [36/40], Loss: 0.6301\n",
      "ðŸ”¥ Epoch [1/20] -> Loss: 0.2839, Accuracy: 90.94%\n",
      "Epoch [2/20], Step [1/40], Loss: 0.1918\n",
      "Epoch [2/20], Step [6/40], Loss: 0.4778\n",
      "Epoch [2/20], Step [11/40], Loss: 0.1846\n",
      "Epoch [2/20], Step [16/40], Loss: 0.6642\n",
      "Epoch [2/20], Step [21/40], Loss: 0.0866\n",
      "Epoch [2/20], Step [26/40], Loss: 0.1190\n",
      "Epoch [2/20], Step [31/40], Loss: 0.5692\n",
      "Epoch [2/20], Step [36/40], Loss: 0.0973\n",
      "ðŸ”¥ Epoch [2/20] -> Loss: 0.2393, Accuracy: 93.28%\n",
      "Epoch [3/20], Step [1/40], Loss: 0.3889\n",
      "Epoch [3/20], Step [6/40], Loss: 0.2395\n",
      "Epoch [3/20], Step [11/40], Loss: 0.1369\n",
      "Epoch [3/20], Step [16/40], Loss: 0.4024\n",
      "Epoch [3/20], Step [21/40], Loss: 0.2107\n",
      "Epoch [3/20], Step [26/40], Loss: 0.1888\n",
      "Epoch [3/20], Step [31/40], Loss: 0.0600\n",
      "Epoch [3/20], Step [36/40], Loss: 0.2576\n",
      "ðŸ”¥ Epoch [3/20] -> Loss: 0.2020, Accuracy: 93.44%\n",
      "Epoch [4/20], Step [1/40], Loss: 0.1249\n",
      "Epoch [4/20], Step [6/40], Loss: 0.2155\n",
      "Epoch [4/20], Step [11/40], Loss: 0.0728\n",
      "Epoch [4/20], Step [16/40], Loss: 0.3250\n",
      "Epoch [4/20], Step [21/40], Loss: 0.0670\n",
      "Epoch [4/20], Step [26/40], Loss: 0.1496\n",
      "Epoch [4/20], Step [31/40], Loss: 0.0515\n",
      "Epoch [4/20], Step [36/40], Loss: 0.2152\n",
      "ðŸ”¥ Epoch [4/20] -> Loss: 0.1764, Accuracy: 93.44%\n",
      "Epoch [5/20], Step [1/40], Loss: 0.2342\n",
      "Epoch [5/20], Step [6/40], Loss: 0.0779\n",
      "Epoch [5/20], Step [11/40], Loss: 0.1842\n",
      "Epoch [5/20], Step [16/40], Loss: 0.0708\n",
      "Epoch [5/20], Step [21/40], Loss: 0.0881\n",
      "Epoch [5/20], Step [26/40], Loss: 0.2008\n",
      "Epoch [5/20], Step [31/40], Loss: 0.2125\n",
      "Epoch [5/20], Step [36/40], Loss: 0.2504\n",
      "ðŸ”¥ Epoch [5/20] -> Loss: 0.1653, Accuracy: 94.14%\n",
      "Epoch [6/20], Step [1/40], Loss: 0.0951\n",
      "Epoch [6/20], Step [6/40], Loss: 0.1777\n",
      "Epoch [6/20], Step [11/40], Loss: 0.1471\n",
      "Epoch [6/20], Step [16/40], Loss: 0.1837\n",
      "Epoch [6/20], Step [21/40], Loss: 0.1200\n",
      "Epoch [6/20], Step [26/40], Loss: 0.2603\n",
      "Epoch [6/20], Step [31/40], Loss: 0.1756\n",
      "Epoch [6/20], Step [36/40], Loss: 0.1172\n",
      "ðŸ”¥ Epoch [6/20] -> Loss: 0.1724, Accuracy: 93.52%\n",
      "Epoch [7/20], Step [1/40], Loss: 0.0335\n",
      "Epoch [7/20], Step [6/40], Loss: 0.2281\n",
      "Epoch [7/20], Step [11/40], Loss: 0.0289\n",
      "Epoch [7/20], Step [16/40], Loss: 0.1709\n",
      "Epoch [7/20], Step [21/40], Loss: 0.1060\n",
      "Epoch [7/20], Step [26/40], Loss: 0.4495\n",
      "Epoch [7/20], Step [31/40], Loss: 0.0478\n",
      "Epoch [7/20], Step [36/40], Loss: 0.0903\n",
      "ðŸ”¥ Epoch [7/20] -> Loss: 0.1467, Accuracy: 95.47%\n",
      "Epoch [8/20], Step [1/40], Loss: 0.0413\n",
      "Epoch [8/20], Step [6/40], Loss: 0.0596\n",
      "Epoch [8/20], Step [11/40], Loss: 0.3064\n",
      "Epoch [8/20], Step [16/40], Loss: 0.2623\n",
      "Epoch [8/20], Step [21/40], Loss: 0.1873\n",
      "Epoch [8/20], Step [26/40], Loss: 0.3517\n",
      "Epoch [8/20], Step [31/40], Loss: 0.0646\n",
      "Epoch [8/20], Step [36/40], Loss: 0.1658\n",
      "ðŸ”¥ Epoch [8/20] -> Loss: 0.1577, Accuracy: 94.30%\n",
      "Epoch [9/20], Step [1/40], Loss: 0.1927\n",
      "Epoch [9/20], Step [6/40], Loss: 0.1808\n",
      "Epoch [9/20], Step [11/40], Loss: 0.0339\n",
      "Epoch [9/20], Step [16/40], Loss: 0.2387\n",
      "Epoch [9/20], Step [21/40], Loss: 0.3278\n",
      "Epoch [9/20], Step [26/40], Loss: 0.0912\n",
      "Epoch [9/20], Step [31/40], Loss: 0.0441\n",
      "Epoch [9/20], Step [36/40], Loss: 0.0887\n",
      "ðŸ”¥ Epoch [9/20] -> Loss: 0.1336, Accuracy: 95.39%\n",
      "Epoch [10/20], Step [1/40], Loss: 0.1286\n",
      "Epoch [10/20], Step [6/40], Loss: 0.0300\n",
      "Epoch [10/20], Step [11/40], Loss: 0.1909\n",
      "Epoch [10/20], Step [16/40], Loss: 0.0506\n",
      "Epoch [10/20], Step [21/40], Loss: 0.1160\n",
      "Epoch [10/20], Step [26/40], Loss: 0.2119\n",
      "Epoch [10/20], Step [31/40], Loss: 0.0441\n",
      "Epoch [10/20], Step [36/40], Loss: 0.0611\n",
      "ðŸ”¥ Epoch [10/20] -> Loss: 0.1282, Accuracy: 95.78%\n",
      "Epoch [11/20], Step [1/40], Loss: 0.0513\n",
      "Epoch [11/20], Step [6/40], Loss: 0.2193\n",
      "Epoch [11/20], Step [11/40], Loss: 0.1003\n",
      "Epoch [11/20], Step [16/40], Loss: 0.1650\n",
      "Epoch [11/20], Step [21/40], Loss: 0.2682\n",
      "Epoch [11/20], Step [26/40], Loss: 0.1023\n",
      "Epoch [11/20], Step [31/40], Loss: 0.0494\n",
      "Epoch [11/20], Step [36/40], Loss: 0.0277\n",
      "ðŸ”¥ Epoch [11/20] -> Loss: 0.1368, Accuracy: 95.86%\n",
      "Epoch [12/20], Step [1/40], Loss: 0.0948\n",
      "Epoch [12/20], Step [6/40], Loss: 0.2090\n",
      "Epoch [12/20], Step [11/40], Loss: 0.0418\n",
      "Epoch [12/20], Step [16/40], Loss: 0.0656\n",
      "Epoch [12/20], Step [21/40], Loss: 0.1188\n",
      "Epoch [12/20], Step [26/40], Loss: 0.0783\n",
      "Epoch [12/20], Step [31/40], Loss: 0.1890\n",
      "Epoch [12/20], Step [36/40], Loss: 0.0461\n",
      "ðŸ”¥ Epoch [12/20] -> Loss: 0.1270, Accuracy: 96.33%\n",
      "Epoch [13/20], Step [1/40], Loss: 0.1106\n",
      "Epoch [13/20], Step [6/40], Loss: 0.0316\n",
      "Epoch [13/20], Step [11/40], Loss: 0.1939\n",
      "Epoch [13/20], Step [16/40], Loss: 0.1209\n",
      "Epoch [13/20], Step [21/40], Loss: 0.0797\n",
      "Epoch [13/20], Step [26/40], Loss: 0.0676\n",
      "Epoch [13/20], Step [31/40], Loss: 0.0905\n",
      "Epoch [13/20], Step [36/40], Loss: 0.1088\n",
      "ðŸ”¥ Epoch [13/20] -> Loss: 0.1162, Accuracy: 96.09%\n",
      "Epoch [14/20], Step [1/40], Loss: 0.0478\n",
      "Epoch [14/20], Step [6/40], Loss: 0.2830\n",
      "Epoch [14/20], Step [11/40], Loss: 0.0687\n",
      "Epoch [14/20], Step [16/40], Loss: 0.0717\n",
      "Epoch [14/20], Step [21/40], Loss: 0.0552\n",
      "Epoch [14/20], Step [26/40], Loss: 0.1899\n",
      "Epoch [14/20], Step [31/40], Loss: 0.1890\n",
      "Epoch [14/20], Step [36/40], Loss: 0.0342\n",
      "ðŸ”¥ Epoch [14/20] -> Loss: 0.1196, Accuracy: 95.86%\n",
      "Epoch [15/20], Step [1/40], Loss: 0.1714\n",
      "Epoch [15/20], Step [6/40], Loss: 0.1110\n",
      "Epoch [15/20], Step [11/40], Loss: 0.1525\n",
      "Epoch [15/20], Step [16/40], Loss: 0.0677\n",
      "Epoch [15/20], Step [21/40], Loss: 0.1169\n",
      "Epoch [15/20], Step [26/40], Loss: 0.0416\n",
      "Epoch [15/20], Step [31/40], Loss: 0.0565\n",
      "Epoch [15/20], Step [36/40], Loss: 0.1321\n",
      "ðŸ”¥ Epoch [15/20] -> Loss: 0.1085, Accuracy: 96.72%\n",
      "Epoch [16/20], Step [1/40], Loss: 0.0364\n",
      "Epoch [16/20], Step [6/40], Loss: 0.0298\n",
      "Epoch [16/20], Step [11/40], Loss: 0.0900\n",
      "Epoch [16/20], Step [16/40], Loss: 0.1225\n",
      "Epoch [16/20], Step [21/40], Loss: 0.2326\n",
      "Epoch [16/20], Step [26/40], Loss: 0.1105\n",
      "Epoch [16/20], Step [31/40], Loss: 0.0902\n",
      "Epoch [16/20], Step [36/40], Loss: 0.0276\n",
      "ðŸ”¥ Epoch [16/20] -> Loss: 0.0979, Accuracy: 96.95%\n",
      "Epoch [17/20], Step [1/40], Loss: 0.0208\n",
      "Epoch [17/20], Step [6/40], Loss: 0.1047\n",
      "Epoch [17/20], Step [11/40], Loss: 0.0395\n",
      "Epoch [17/20], Step [16/40], Loss: 0.0990\n",
      "Epoch [17/20], Step [21/40], Loss: 0.0272\n",
      "Epoch [17/20], Step [26/40], Loss: 0.0175\n",
      "Epoch [17/20], Step [31/40], Loss: 0.2137\n",
      "Epoch [17/20], Step [36/40], Loss: 0.0766\n",
      "ðŸ”¥ Epoch [17/20] -> Loss: 0.1109, Accuracy: 96.09%\n",
      "Epoch [18/20], Step [1/40], Loss: 0.0915\n",
      "Epoch [18/20], Step [6/40], Loss: 0.1480\n",
      "Epoch [18/20], Step [11/40], Loss: 0.0590\n",
      "Epoch [18/20], Step [16/40], Loss: 0.1979\n",
      "Epoch [18/20], Step [21/40], Loss: 0.0511\n",
      "Epoch [18/20], Step [26/40], Loss: 0.0247\n",
      "Epoch [18/20], Step [31/40], Loss: 0.3297\n",
      "Epoch [18/20], Step [36/40], Loss: 0.0210\n",
      "ðŸ”¥ Epoch [18/20] -> Loss: 0.1059, Accuracy: 96.64%\n",
      "Epoch [19/20], Step [1/40], Loss: 0.0462\n",
      "Epoch [19/20], Step [6/40], Loss: 0.1150\n",
      "Epoch [19/20], Step [11/40], Loss: 0.0740\n",
      "Epoch [19/20], Step [16/40], Loss: 0.1685\n",
      "Epoch [19/20], Step [21/40], Loss: 0.0183\n",
      "Epoch [19/20], Step [26/40], Loss: 0.0412\n",
      "Epoch [19/20], Step [31/40], Loss: 0.1133\n",
      "Epoch [19/20], Step [36/40], Loss: 0.0781\n",
      "ðŸ”¥ Epoch [19/20] -> Loss: 0.1053, Accuracy: 96.48%\n",
      "Epoch [20/20], Step [1/40], Loss: 0.0605\n",
      "Epoch [20/20], Step [6/40], Loss: 0.0254\n",
      "Epoch [20/20], Step [11/40], Loss: 0.0980\n",
      "Epoch [20/20], Step [16/40], Loss: 0.0816\n",
      "Epoch [20/20], Step [21/40], Loss: 0.0901\n",
      "Epoch [20/20], Step [26/40], Loss: 0.0185\n",
      "Epoch [20/20], Step [31/40], Loss: 0.4008\n",
      "Epoch [20/20], Step [36/40], Loss: 0.0637\n",
      "ðŸ”¥ Epoch [20/20] -> Loss: 0.1080, Accuracy: 96.72%\n",
      "âœ… Model saved successfully!\n",
      "ðŸŽ¯ Test Accuracy: 92.52%\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "92.5233644859813"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# # Example Data (Ensure your real data is loaded correctly)\n",
    "# spectrogram_data = np.random.rand(1000, 28, 5, 1280)  # Simulated input\n",
    "# y = np.random.randint(0, 2, (1000,))  # Simulated binary classification labels\n",
    "\n",
    "# # Train-Test Split\n",
    "# X_train, X_test, y_train, y_test = train_test_split(spectrogram_data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# # Convert to PyTorch Tensors\n",
    "# X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "# X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "# y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "# y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# # Create DataLoader objects\n",
    "batch_size = 32\n",
    "# train_dataset = TensorDataset(X_train, y_train)\n",
    "# test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_data, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, Loss Function, and Optimizer\n",
    "model = TGCNN().to(device)  # Move model to GPU if available\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Parameters\n",
    "num_epochs = 20  # Adjust based on dataset size\n",
    "print_interval = 5  # Print every 5 batches\n",
    "\n",
    "# ðŸš€ Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        \n",
    "        loss = criterion(outputs, targets)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # Optional: Gradient clipping\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        if batch_idx % print_interval == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"ðŸ”¥ Epoch [{epoch+1}/{num_epochs}] -> Loss: {total_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"TGCNN_model.pth\")\n",
    "print(\"âœ… Model saved successfully!\")\n",
    "\n",
    "# ðŸš€ Evaluation Function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"ðŸŽ¯ Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Run Evaluation\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")\n",
    "\n",
    "# Example Data (Ensure your real data is loaded correctly)\n",
    "spectrogram_data = np.random.rand(1000, 28, 5, 1280)  # Simulated input\n",
    "y = np.random.randint(0, 2, (1000,))  # Simulated binary classification labels\n",
    "\n",
    "# Train-Test Split\n",
    "X_train, X_test, y_train, y_test = train_test_split(spectrogram_data, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Convert to PyTorch Tensors\n",
    "X_train = torch.tensor(X_train, dtype=torch.float32).to(device)\n",
    "X_test = torch.tensor(X_test, dtype=torch.float32).to(device)\n",
    "y_train = torch.tensor(y_train, dtype=torch.long).to(device)\n",
    "y_test = torch.tensor(y_test, dtype=torch.long).to(device)\n",
    "\n",
    "# Create DataLoader objects\n",
    "batch_size = 32\n",
    "train_dataset = TensorDataset(X_train, y_train)\n",
    "test_dataset = TensorDataset(X_test, y_test)\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Model, Loss Function, and Optimizer\n",
    "model = TGCNN().to(device)  # Move model to GPU if available\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Training Parameters\n",
    "num_epochs = 20  # Adjust based on dataset size\n",
    "print_interval = 5  # Print every 5 batches\n",
    "\n",
    "# ðŸš€ Training Loop\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # Set model to training mode\n",
    "    total_loss = 0\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    \n",
    "    for batch_idx, (inputs, targets) in enumerate(train_loader):\n",
    "        inputs, targets = inputs.to(device), targets.to(device)\n",
    "        \n",
    "        optimizer.zero_grad()  # Reset gradients\n",
    "        outputs = model(inputs)  # Forward pass\n",
    "        \n",
    "        loss = criterion(outputs, targets)  # Compute loss\n",
    "        loss.backward()  # Backpropagation\n",
    "        \n",
    "        nn.utils.clip_grad_norm_(model.parameters(), max_norm=5)  # Optional: Gradient clipping\n",
    "        optimizer.step()  # Update weights\n",
    "        \n",
    "        total_loss += loss.item()\n",
    "        predicted = torch.argmax(outputs, dim=1)\n",
    "        correct += (predicted == targets).sum().item()\n",
    "        total += targets.size(0)\n",
    "\n",
    "        if batch_idx % print_interval == 0:\n",
    "            print(f\"Epoch [{epoch+1}/{num_epochs}], Step [{batch_idx+1}/{len(train_loader)}], Loss: {loss.item():.4f}\")\n",
    "\n",
    "    # Print epoch summary\n",
    "    print(f\"ðŸ”¥ Epoch [{epoch+1}/{num_epochs}] -> Loss: {total_loss/len(train_loader):.4f}, Accuracy: {100 * correct / total:.2f}%\")\n",
    "\n",
    "# Save the trained model\n",
    "torch.save(model.state_dict(), \"TGCNN_model.pth\")\n",
    "print(\"âœ… Model saved successfully!\")\n",
    "\n",
    "# ðŸš€ Evaluation Function\n",
    "def evaluate_model(model, test_loader):\n",
    "    model.eval()  # Set model to evaluation mode\n",
    "    correct = 0\n",
    "    total = 0\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    \n",
    "    with torch.no_grad():  # Disable gradient calculation\n",
    "        for inputs, targets in test_loader:\n",
    "            inputs, targets = inputs.to(device), targets.to(device)\n",
    "            outputs = model(inputs)\n",
    "            \n",
    "            predicted = torch.argmax(outputs, dim=1)\n",
    "            correct += (predicted == targets).sum().item()\n",
    "            total += targets.size(0)\n",
    "            \n",
    "            all_preds.extend(predicted.cpu().numpy())\n",
    "            all_labels.extend(targets.cpu().numpy())\n",
    "\n",
    "    accuracy = 100 * correct / total\n",
    "    print(f\"ðŸŽ¯ Test Accuracy: {accuracy:.2f}%\")\n",
    "    return accuracy\n",
    "\n",
    "# Run Evaluation\n",
    "evaluate_model(model, test_loader)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Using device: cpu\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Check for GPU\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
