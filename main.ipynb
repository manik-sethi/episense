{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import glob, os\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import yaml\n",
    "import mne\n",
    "from mne.io import concatenate_raws, read_raw_edf\n",
    "from mne import Epochs\n",
    "from mne.decoding import Scaler\n",
    "from scipy import signal\n",
    "from scipy.signal import butter, lfilter\n",
    "from scipy.signal import ShortTimeFFT\n",
    "from extraction import extract_interictal_preictal\n",
    "from pipeline import Pipeline\n",
    "import torch as tf\n",
    "from torch.utils.data import TensorDataset, DataLoader, random_split, Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating Spectograms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "'base_subject_dir'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 8\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[39m# Load configuration from config.yaml\u001b[39;00m\n\u001b[1;32m      7\u001b[0m config \u001b[39m=\u001b[39m load_config(\u001b[39m\"\u001b[39m\u001b[39mconfig.yaml\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m----> 8\u001b[0m base_subject_dir \u001b[39m=\u001b[39m config[\u001b[39m\"\u001b[39;49m\u001b[39mbase_subject_dir\u001b[39;49m\u001b[39m\"\u001b[39;49m]\n\u001b[1;32m      9\u001b[0m subject_list \u001b[39m=\u001b[39m config[\u001b[39m\"\u001b[39m\u001b[39msubject_range\u001b[39m\u001b[39m\"\u001b[39m]\n\u001b[1;32m     11\u001b[0m \u001b[39m# For testing, process only the first 2 subjects from subject_list\u001b[39;00m\n",
      "\u001b[0;31mKeyError\u001b[0m: 'base_subject_dir'"
     ]
    }
   ],
   "source": [
    "\n",
    "def load_config(config_path):\n",
    "    with open(config_path, 'r') as f:\n",
    "        config = yaml.safe_load(f)\n",
    "    return config\n",
    "\n",
    "# Load configuration from config.yaml\n",
    "config = load_config(\"config.yaml\")\n",
    "base_subject_dir = config[\"base_subject_dir\"]\n",
    "subject_list = config[\"subject_range\"]\n",
    "\n",
    "# For testing, process only the first 2 subjects from subject_list\n",
    "selected_subjects = subject_list[:2]\n",
    "\n",
    "all_time_bins = []  # Will accumulate STFT outputs of shape (22, 5, t_i)\n",
    "all_labels = []     # Will have one label per time bin (already multiplied in the pipeline)\n",
    "all_ranges = {}\n",
    "\n",
    "for subj in subject_list:\n",
    "    subject_folder = f\"chb{subj:02d}\"\n",
    "    summary_file = os.path.join(base_subject_dir, subject_folder, f\"chb{subj:02d}-summary.txt\")\n",
    "    print(f\"\\nProcessing summary file for subject {subject_folder}: {summary_file}\")\n",
    "    \n",
    "    subject_ranges = extract_interictal_preictal(summary_file)\n",
    "    all_ranges[subject_folder] = subject_ranges\n",
    "    \n",
    "    # Process each EDF file for this subject\n",
    "    for edf_fname, ranges in subject_ranges.items():\n",
    "        # print(\"Processing EDF file:\", edf_fname)\n",
    "        # print(\"Ranges:\", ranges)\n",
    "        edf_file = os.path.join(base_subject_dir, subject_folder, edf_fname)\n",
    "        # print(f\"\\nProcessing EDF file: {edf_file}\")\n",
    "        # print(\"Ranges:\", ranges)\n",
    "        \n",
    "        pipe = Pipeline()\n",
    "        pipe.CONFIG(\n",
    "            fname=edf_file,\n",
    "            fs=config[\"fs\"],\n",
    "            window_size=config[\"window_size\"],\n",
    "            overlap=config[\"overlap\"],\n",
    "            f_low=config[\"f_low\"],\n",
    "            f_high=config[\"f_high\"],\n",
    "            ranges_dict=ranges\n",
    "        )\n",
    "        \n",
    "        combined_epochs, epoch_labels = pipe.run_pipeline()\n",
    "        # print(f\"Completed processing {edf_fname}. Number of epoch segments: {len(combined_epochs)}\")\n",
    "        \n",
    "        # ***** The change is here: Instead of looping over epochs and replicating labels, \n",
    "        # simply extend the global lists with the epochs and labels returned by the pipeline.\n",
    "        all_time_bins.extend(combined_epochs)\n",
    "        all_labels.extend(epoch_labels)\n",
    "        # ***** End of change.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Concatenate along the time axis:\n",
    "if all_time_bins:\n",
    "    X = np.concatenate(all_time_bins, axis=-1)  # Final shape: (22, 5, total_time_bins)\n",
    "else:\n",
    "    X = None\n",
    "\n",
    "y = np.array(all_labels)  # y has length equal to the total number of labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0 0 0 ... 0 0 0]\n"
     ]
    }
   ],
   "source": [
    "# Mapping dictionary\n",
    "mapping = {\n",
    "    \"Interictal\": 0,\n",
    "    \"Preictal\": 1\n",
    "}\n",
    "\n",
    "# vectorize the mapping\n",
    "map_func = np.vectorize(mapping.get)\n",
    "numeric_labels = map_func(copy_y)\n",
    "\n",
    "print(numeric_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Creating tensors for ML"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "X_preictal shape: (22, 5, 6048)\n",
      "X_interictal shape: (22, 5, 65845)\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "# Assuming X.shape = (22, 5, 71893) and y.shape = (71893,)\n",
    "X_preictal = []\n",
    "X_interictal = []\n",
    "\n",
    "# Iterate over the timestamps\n",
    "for i in range(X.shape[2]):  \n",
    "    if numeric_labels[i] == 1:  # Preictal\n",
    "        X_preictal.append(X[:, :, i])\n",
    "    else:  # Interictal\n",
    "        X_interictal.append(X[:, :, i])\n",
    "\n",
    "# Convert lists to numpy arrays\n",
    "X_preictal = np.array(X_preictal).transpose(1, 2, 0) if X_preictal else np.empty((22, 5, 0))\n",
    "X_interictal = np.array(X_interictal).transpose(1, 2, 0) if X_interictal else np.empty((22, 5, 0))\n",
    "\n",
    "# Print the shapes for verification\n",
    "print(\"X_preictal shape:\", X_preictal.shape)\n",
    "print(\"X_interictal shape:\", X_interictal.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "x1 = tf.tensor(X)\n",
    "y1 = tf.tensor(numeric_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "x2 = x1.permute(2,0,1)\n",
    "dataset = TensorDataset(x2, y1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_size = int(0.8 * len(dataset))\n",
    "test_size = len(dataset) - train_size\n",
    "train_data, test_data = random_split(dataset, [train_size, test_size])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "torch.Size([22, 5, 71893])\n"
     ]
    }
   ],
   "source": [
    "print(x1.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Pickling the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "# Define file names\n",
    "dataset_file = \"dataset.pkl\"\n",
    "\n",
    "\n",
    "# Save the preictal and interictal data\n",
    "with open(dataset_file, \"wb\") as f:\n",
    "    pickle.dump(dataset, f)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Batch images shape: torch.Size([10, 3, 32, 32]) Batch labels shape: torch.Size([10])\n",
      "Dataset saved as random_dataset.pt\n"
     ]
    }
   ],
   "source": [
    "\n",
    "class RandomDataset(Dataset):\n",
    "    def __init__(self, num_samples=100, channels=3, height=32, width=32, num_classes=10):\n",
    "        self.num_samples = num_samples\n",
    "        # Generate random image data (e.g., resembling 32x32 RGB images)\n",
    "        self.data = tf.randn(num_samples, channels, height, width)\n",
    "        # Generate random labels between 0 and num_classes-1\n",
    "        self.labels = tf.randint(0, num_classes, (num_samples,))\n",
    "    \n",
    "    def __len__(self):\n",
    "        return self.num_samples\n",
    "    \n",
    "    def __getitem__(self, idx):\n",
    "        return self.data[idx], self.labels[idx]\n",
    "\n",
    "# Create the dataset and optionally inspect one batch\n",
    "dataset = RandomDataset(num_samples=100)\n",
    "loader = DataLoader(dataset, batch_size=10, shuffle=True)\n",
    "\n",
    "# Print the shape of one batch as a quick test\n",
    "for images, labels in loader:\n",
    "    print(\"Batch images shape:\", images.shape, \"Batch labels shape:\", labels.shape)\n",
    "    break\n",
    "\n",
    "# Save the dataset to disk using torch.save\n",
    "dataset_filename = \"random_dataset.pt\"\n",
    "tf.save(dataset, dataset_filename)\n",
    "print(f\"Dataset saved as {dataset_filename}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
